{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedb850e-73dd-4e9e-aaa0-4fd35d13c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "from pdb import set_trace\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0515987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Is cuda available?: True\n",
      "cuda version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the GPU is available and store it as a variable so tensors can be moved to it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print('Is cuda available?:', torch.cuda.is_available())\n",
    "print('cuda version:', torch.version.cuda)\n",
    "dev = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d080d96-3db7-4b7a-8e17-3c711e919f96",
   "metadata": {},
   "source": [
    "### Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03e9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the summary file name\n",
    "Time_Stamp = datetime.datetime.now().strftime(\"%Y_%m%d%_H%M\")\n",
    "\n",
    "# Create run name\n",
    "Run_Name = 'Oct22_protANDmRNA_weights_residual' # change run_name before running\n",
    "\n",
    "# Paths\n",
    "dataset_folder_path = '/home/ec2-user/ProteinLocalization/Datasets/' # dataset folder with all the proteome and mRNA data\n",
    "summary_folder_path = '/home/ec2-user/ProteinLocalization/Summary/' # summary folder path to store summary reports\n",
    "model_folder_path = '/home/ec2-user/ProteinLocalization/Model exports/' # model folder path to store models\n",
    "figure_folder_path = '/home/ec2-user/ProteinLocalization/Figure exports/' # figure folder path to store figures\n",
    "performance_folder_path = '/home/ec2-user/ProteinLocalization/Performance DF exports/' # performance folder path to store performance dataframes\n",
    "\n",
    "testset_directory = dataset_folder_path+'Test set raw data/'\n",
    "johansson_protein_path = testset_directory+'jo_protein_log2.csv'\n",
    "johansson_mRNA_path = testset_directory+'jo_mrna_dropna.csv'\n",
    "mertins_protein_path = testset_directory+'me_protein_dropna.csv'\n",
    "mertins_mRNA_path = testset_directory+'me_rna_dropna.csv'\n",
    "\n",
    "\n",
    "\n",
    "# Define the Model\n",
    "Model = 'CustomNN-stride2' # 'ResNet18', 'CustomNN-reg', and 'CustomNN-stride2' are the options\n",
    "\n",
    "# Define loss function\n",
    "LossFunc_Name = 'CrossEntropy' # 'CrossEntropy', 'Focal' are the options\n",
    "\n",
    "# Dataset\n",
    "Set = 'Protein + mRNA' # Define the dataset to use, 'Protein', 'mRNA', or 'Protein + mRNA' are the options\n",
    "\n",
    "# Image size\n",
    "Canvas_Size = 18\n",
    "\n",
    "# Seed number\n",
    "Seed = 43\n",
    "\n",
    "# Validation fraction\n",
    "Validation_Fraction = 0.2\n",
    "\n",
    "# Standard deviation for add-noise transformation\n",
    "StandardDeviation = 0.05\n",
    "\n",
    "# Learning rate scheduler\n",
    "LrScheduler = True\n",
    "Learner_rate = 5e-5\n",
    "\n",
    "# Synthetic data\n",
    "Bayesian = True\n",
    "Normal = False\n",
    "\n",
    "# Training set imbalance sampling method, can only choose one\n",
    "ImbalanceSampler = True\n",
    "\n",
    "# Optimizer\n",
    "Weight_Decay = 0.001 # L2 regulator\n",
    "#  it works by adding a penalty to the loss function, which discourages large weights in the model; penalizes for too many weights - helps prevent overfitting\n",
    "#  the penalty is calculated as weight_decay * weight^2, and it's added to the loss.\n",
    "#  1e-4 or 1e-3 gives higher acc\n",
    "\n",
    "Momentum = 0.9\n",
    "#  specific for SGD optimizer, not applicable when using Adam optimizer\n",
    "#  a way to smooth noise that is passed to the optimizer, 0.9\n",
    "#  momentum is deterimental without label smoothing\n",
    "\n",
    "# Loss Function (CrossEntropyLoss)\n",
    "Label_Smoothing = 0.05\n",
    "#  sets the target of the loss function to something greater than 0 and less than 1\n",
    "#  helps prevent overfitting\n",
    "\n",
    "# Transformation\n",
    "TransformOrNot = True\n",
    "\n",
    "# Class weight to loss function\n",
    "ApplyClassWeightToLoss = True\n",
    "#  makes the loss weights equal to the fraction of each category label\n",
    "#  note, a layer is added to the model so the outputs of the model are equal to the number of categories\n",
    "\n",
    "# Batch size\n",
    "batch = 64\n",
    "\n",
    "# Epoch amount\n",
    "epochs = 200\n",
    "\n",
    "# Warning\n",
    "if Bayesian == Normal:\n",
    "    raise ValueError('Can only choose one synthetic data method')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcb615",
   "metadata": {},
   "source": [
    "### Protein, mRNA dataframe, localization label set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af90684-fea6-47b5-8079-0dd01492c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the proteomics data and only keep genes (rows) that are fully quantified\n",
    "PFP = 'kr_pro_raw.csv' # proteomics file path, normalized by pool, log2 transformed.\n",
    "PD = pd.read_csv(dataset_folder_path+PFP)\n",
    "\n",
    "# Data set wrangling\n",
    "PD.index = PD.loc[:,'Gene']\n",
    "PD = PD.loc[:,PD.columns!='Gene']\n",
    "PD.dropna(inplace=True)\n",
    "\n",
    "# Specific for krug raw ddamsproteomics, these 3 tumors are not in transcriptome data\n",
    "PD = PD.loc[:,PD.columns!='X11BR057']\n",
    "PD = PD.loc[:,PD.columns!='X11BR076']\n",
    "PD = PD.loc[:,PD.columns!='X11BR078']\n",
    "\n",
    "# Nate changes\n",
    "# undo log2 transform\n",
    "PD = PD**2\n",
    "# row normalize\n",
    "for index in PD.index:\n",
    "    PD.loc[index,:] = PD.loc[index,:]/np.average(PD.loc[index,:])\n",
    "# column normalize\n",
    "for column in PD.columns:\n",
    "    PD.loc[:,column] = PD.loc[:,column]/np.median(PD.loc[:,column])\n",
    "# Replace zero values with a small positive number\n",
    "PD = PD.replace(0, 2**(-10))\n",
    "# log transform\n",
    "PD = np.log2(PD)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(PD.values.flatten().tolist())\n",
    "\n",
    "#Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "#   the normalization factor is computed as the 2.5th to 97.5th percentile of all data points in the matrix\n",
    "#   This does not force the distributions for each gene to be the same because the same normalization factor is used for all genes\n",
    "#     Not a normalization factor based on the distribution of each gene\n",
    "#PD = (PD - percentile_low) / (percentile_high - percentile_low)\n",
    "PD = (PD) / (percentile_high - percentile_low)\n",
    "\n",
    "# Open the mRNA data and only keep genes (rows) that are fully quantified\n",
    "MFP = 'kr_rna_raw.csv' # mRNA file path, gene centric median normalized, log2 transformed\n",
    "MD = pd.read_csv(dataset_folder_path + MFP)\n",
    "\n",
    "\n",
    "# Data set wrangling\n",
    "MD.index = MD.loc[:,'Gene']\n",
    "MD = MD.loc[:,MD.columns!='Gene']\n",
    "MD = MD.drop_duplicates()\n",
    "MD.dropna(inplace=True)\n",
    "\n",
    "# undo log2 transform\n",
    "MD = MD**2\n",
    "\n",
    "# row normalize\n",
    "for index in MD.index:\n",
    "    MD.loc[index,:] = MD.loc[index,:]/np.average(MD.loc[index,:])\n",
    "\n",
    "# column normalize\n",
    "for column in MD.columns:\n",
    "    MD.loc[:,column] = MD.loc[:,column]/np.median(MD.loc[:,column])\n",
    "\n",
    "# Replace zero values with a small positive number\n",
    "MD = MD.replace(0, 2**(-10))\n",
    "\n",
    "# log transform\n",
    "MD = np.log2(MD)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(MD.values.flatten().tolist())\n",
    "\n",
    "# Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "#MD = (MD - percentile_low) / (percentile_high - percentile_low)\n",
    "MD = (MD) / (percentile_high - percentile_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d093503b-cbc4-47ee-a7a6-3d9f3b371687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the labels data\n",
    "LFP = 'SubCellBarcode.MCF7.txt'\n",
    "LD = pd.read_csv(filepath_or_buffer=dataset_folder_path+LFP,sep='\\t')\n",
    "\n",
    "# Data set wrangling\n",
    "LD.index = LD.loc[:,'Protein']\n",
    "LD = LD.loc[:,LD.columns!='Protein']\n",
    "\n",
    "# Remove unclassified class\n",
    "NotUnclassInd = LD.loc[:,'Localization'] != 'Unclassified'\n",
    "LD = LD.loc[NotUnclassInd,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415621a6-6435-45ba-b7ef-3457080d6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes in Krug dataset\n",
      "4839\n",
      "4839\n",
      "4839\n"
     ]
    }
   ],
   "source": [
    "# Keep only genes (rows) are presented in proteome, mRNA and localization data sets\n",
    "IntersectingGenes = [value for value in PD.index if ((value in MD.index) & (value in LD.index))]\n",
    "PD = PD.loc[IntersectingGenes,:]\n",
    "MD = MD.loc[IntersectingGenes,:]\n",
    "LD = LD.loc[IntersectingGenes,:]\n",
    "\n",
    "# Sanity check for the number of genes in each dataframe\n",
    "print('Number of genes in Krug dataset')\n",
    "print(len(PD.index))\n",
    "print(len(MD.index))\n",
    "print(len(LD.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a83d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krug proteome label distribution:\n",
      "Localization\n",
      "Cytosol         2121\n",
      "Nuclear         1392\n",
      "Secretory        991\n",
      "Mitochondria     335\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Krug dataset label distribution\n",
    "count = LD['Localization'].value_counts()\n",
    "\n",
    "# Print out the label distribution\n",
    "print('Krug proteome label distribution:')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfb7cc",
   "metadata": {},
   "source": [
    "### Set seed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688d65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed) # set random seed for python\n",
    "    np.random.seed(seed) # set random seed for numpy\n",
    "    torch.manual_seed(seed) # set random seed for CPU\n",
    "    rs = RandomState(MT19937(SeedSequence(seed))) # seed for numpy\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed) # set random seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True # set to True to get reproducible results\n",
    "    torch.backends.cudnn.benchmark = False # set to False to get reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60102591",
   "metadata": {},
   "source": [
    "### Synthetic data using Baysian GGM or norm dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33935c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to ensure reproducibility\n",
    "set_seed(Seed)\n",
    "\n",
    "if Bayesian:\n",
    "\n",
    "    if Set == 'Protein' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        PD_bgm = BayesianGaussianMixture(n_components=5, random_state=42) # Assuming the maximum number of clusters in dataset is 5\n",
    "        PD_T = PD.T\n",
    "        PD_bgm.fit(PD_T)\n",
    "    \n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'Protein':\n",
    "            synthetic_PD, _ = PD_bgm.sample(int(Canvas_Size*Canvas_Size-PD.columns.size))\n",
    "        elif Set == 'Protein + mRNA':\n",
    "            synthetic_PD, _ = PD_bgm.sample(int((Canvas_Size*Canvas_Size-PD.columns.size*2)/2))\n",
    "    \n",
    "        # Transpose back before merging\n",
    "        synthetic_PD = synthetic_PD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        synthetic_PD = pd.DataFrame(synthetic_PD.tolist(), index=PD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        PD = pd.concat([PD, synthetic_PD], axis=1)\n",
    "\n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        \n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(PD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(PD.columns) == Canvas_Size*Canvas_Size\n",
    "\n",
    "        print(f'{Set} - Bayesian PD: {len(PD.columns)}')\n",
    "      \n",
    "    if Set == 'mRNA' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        MD_bgm = BayesianGaussianMixture(n_components=5, random_state=43) # Assuming the maximum number of clusters in dataset is 5\n",
    "        MD_T = MD.T\n",
    "        MD_bgm.fit(MD_T)\n",
    "\n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'mRNA':\n",
    "            synthetic_MD, _ = MD_bgm.sample(int(Canvas_Size*Canvas_Size-MD.columns.size))\n",
    "        elif Set == 'Protein + mRNA':\n",
    "            synthetic_MD, _ = MD_bgm.sample(int((Canvas_Size*Canvas_Size-MD.columns.size*2)/2))\n",
    "\n",
    "        # Transpose back before merging\n",
    "        synthetic_MD = synthetic_MD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        synthetic_MD = pd.DataFrame(synthetic_MD.tolist(), index=MD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        MD = pd.concat([MD, synthetic_MD], axis=1)\n",
    "         \n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(MD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(MD.columns) == Canvas_Size*Canvas_Size\n",
    "        print(f'{Set} - Bayesian MD: {len(MD.columns)}')\n",
    "\n",
    "elif Normal: # use normal distritbuion\n",
    "    \n",
    "    # Helper function\n",
    "    def SyntheticNormDistAbund(row, std_dev):\n",
    "        return [np.random.normal(loc=abund, scale=std_dev) for abund in row]\n",
    "\n",
    "    if Set == 'Protein' or Set == 'Protein + mRNA':\n",
    "        # Calculate the standard deviation of each row in PD and MD\n",
    "        PD_gene_std = PD.std(axis=1)\n",
    "    \n",
    "        # Randomly subset X tumors from PD, apply the same subset tumors to MD\n",
    "        if Set == 'Protein':\n",
    "            subset_columns = np.random.choice(PD.columns, int(Canvas_Size*Canvas_Size-PD.columns.size), replace=False)\n",
    "            \n",
    "        if Set == 'Protein + mRNA':\n",
    "            subset_columns = np.random.choice(PD.columns, int((Canvas_Size*Canvas_Size-PD.columns.size*2)/2), replace=False)\n",
    "        \n",
    "        subset_PD = PD[subset_columns]\n",
    "        # Generate X numbers for each gene using normal distribution \n",
    "        # mean = individual tumors' abundance\n",
    "        # std = std of total tumors' abunbdance\n",
    "\n",
    "        # Applying the function to each row in subset PD and MD\n",
    "        synthetic_PD = subset_PD.apply(lambda row: SyntheticNormDistAbund(row, PD_gene_std[row.name]), axis=1)\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        synthetic_PD = pd.DataFrame(synthetic_PD.tolist(), index=PD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        PD = pd.concat([PD, synthetic_PD], axis=1)\n",
    "\n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(PD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(PD.columns) == Canvas_Size*Canvas_Size\n",
    "        print(f\"{Set} - Normal PD: {len(PD.columns)}\")\n",
    "\n",
    "    if Set == 'mRNA' or Set == 'Protein + mRNA':\n",
    "        # Calculate the standard deviation of each row in PD and MD\n",
    "        MD_gene_std = MD.std(axis=1)\n",
    "\n",
    "        # Randomly subset X tumors from MD\n",
    "        if Set == 'mRNA':\n",
    "            subset_columns = np.random.choice(MD.columns, int(Canvas_Size*Canvas_Size-MD.columns.size), replace=False)\n",
    "            subset_MD = MD[subset_columns]\n",
    "        elif Set == 'Protein + mRNA':\n",
    "            subset_MD = MD[subset_columns]\n",
    "        \n",
    "        # Generate X numbers for each gene using normal distribution \n",
    "        # mean = individual tumors' abundance\n",
    "        # std = std of total tumors' abunbdance\n",
    "\n",
    "        # Applying the function to each row in subset PD and MD\n",
    "        synthetic_MD = subset_MD.apply(lambda row: SyntheticNormDistAbund(row, MD_gene_std[row.name]), axis=1)\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        synthetic_MD = pd.DataFrame(synthetic_MD.tolist(), index=MD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        MD = pd.concat([MD, synthetic_MD], axis=1)\n",
    "        \n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(MD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(MD.columns) == Canvas_Size*Canvas_Size\n",
    "\n",
    "        print(f\"{Set} - Normal MD: {len(MD.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print('No synthetic data generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ff743",
   "metadata": {},
   "source": [
    "### Canvas and RGB tensor generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave_arrays(set=None, PD=None, MD=None):\n",
    "    \"\"\"\n",
    "    Interleave the values from PD, MD and RD arrays if using both datasets and feature engineering, otherwise sort the PD or MD array.\n",
    "    \n",
    "    Parameters:\n",
    "    set (str): Dataset used for plotting ('Protein', 'mRNA', 'Protein + mRNA').\n",
    "    PD (pd.DataFrame): Array containing normalized protein abundance data.\n",
    "    MD (pd.DataFrame): Array containing normalized mRNA abundance data.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Interleaved array of PD and MD values if both are provided, otherwise sorted PD or MD array.\n",
    "    \"\"\"\n",
    "    def sort_and_interleave(PD, MD=None):\n",
    "        sum_abundance = PD + (MD if MD is not None else 0)\n",
    "        sorted_indices = np.argsort(sum_abundance)[::-1]\n",
    "        PD_sorted = np.take_along_axis(PD.values, sorted_indices, axis=1)\n",
    "        if MD is not None:\n",
    "            MD_sorted = np.take_along_axis(MD.values, sorted_indices, axis=1)\n",
    "            return PD_sorted, MD_sorted\n",
    "        return PD_sorted, None\n",
    "\n",
    "    if set == 'Protein + mRNA':\n",
    "        assert PD.shape == MD.shape, \"PD and MD dataframes must have the same shape.\"\n",
    "        PD_sorted, MD_sorted = sort_and_interleave(PD, MD)\n",
    "        \n",
    "        interleaved_array = np.empty((len(PD_sorted), Canvas_Size*Canvas_Size), dtype=PD_sorted.dtype)\n",
    "        interleaved_array[:, 0::2] = PD_sorted\n",
    "        interleaved_array[:, 1::2] = MD_sorted\n",
    "\n",
    "        return interleaved_array\n",
    "\n",
    "    elif set == 'Protein':\n",
    "        assert PD is not None and MD is None, \"PD dataframe must be provided.\"\n",
    "        PD_sorted, _ = sort_and_interleave(PD)\n",
    "        return PD_sorted\n",
    "\n",
    "    elif set == 'mRNA':\n",
    "        assert MD is not None and PD is None, \"MD dataframe must be provided.\"\n",
    "        MD_sorted, _ = sort_and_interleave(MD)\n",
    "        return MD_sorted\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Set must be 'Protein', 'mRNA', or 'Protein + mRNA'\")\n",
    "\n",
    "def gen_horizontal_coords(n, num_points):\n",
    "    \"\"\"\n",
    "    Generate coordinates for a horizontal pattern starting from the top-left corner.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Size of the canvas (n x n).\n",
    "    num_points (int): Number of points to generate in the horizontal pattern.\n",
    "\n",
    "    Returns:\n",
    "    list: List of (x, y) coordinates in horizontal order.\n",
    "    \"\"\"\n",
    "    coords = [(x, y) for x in range(n) for y in range(n)]\n",
    "    return coords[:num_points]\n",
    "\n",
    "def create_rgb_tensors(set, PD, MD):\n",
    "    \"\"\"\n",
    "    Create RGB tensors from PD and MD data using the specified pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    PD (pd.DataFrame): DataFrame containing normalized protein abundance data.\n",
    "    MD (pd.DataFrame): DataFrame containing normalized mRNA abundance data.\n",
    "    pattern (str): Pattern to use for arranging the data ('spiral').\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: RGB tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # PD is a row of the protein abundance dataframe\n",
    "    # MD is a row of the protein abundance dataframe\n",
    "\n",
    "    final_array = interleave_arrays(set, PD, MD)\n",
    "    final_array = final_array.flatten()\n",
    "    num_pixels = len(final_array)\n",
    "\n",
    "    # Generate coordinates based on the pattern, spiral or horizontal arrangment\n",
    "    coords = gen_horizontal_coords(Canvas_Size, num_pixels)\n",
    "\n",
    "    # Create RGB tensor\n",
    "    rgb_tensor = np.zeros((Canvas_Size, Canvas_Size, 3), dtype=np.uint8)\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        if i < len(final_array):\n",
    "            value = final_array[i]\n",
    "            fraction_of_color_range = (value-(-1))/2\n",
    "            # Assign colors based on the value, if abundance > 1, set it to red, if abundance < 0, set it to blue.\n",
    "            if value > 1:\n",
    "                #rgb_tensor[x, y] = [255, 0, 0]  # Red for value > 1\n",
    "                rgb_tensor[x, y] = [255, 0, 0]  # Blue for value < 0\n",
    "            elif value < -1:\n",
    "                rgb_tensor[x, y] = [0, 0, 255]  # Blue for value < 0\n",
    "            elif value == 0:\n",
    "                rgb_tensor[x, y] = [0, 0, 0]\n",
    "            else:\n",
    "                rgb_tensor[x, y] = [255 * fraction_of_color_range, 0, 255 * (1 - fraction_of_color_range)]\n",
    "                \n",
    "    return torch.from_numpy(rgb_tensor.transpose(2, 0, 1))  # Convert to CHW format for PyTorch\n",
    "\n",
    "def annotate_canvas(set, PD, MD, rgb_tensor, gene_name, label, numbering=True, label_type='order'):\n",
    "    \"\"\"\n",
    "    Plot the canvas with RGB tensor and optionally number the pixels.\n",
    "\n",
    "    Parameters:\n",
    "    set (str): Dataset used for plotting ('Protein', 'mRNA', 'Protein + mRNA').\n",
    "    PD (pd.DataFrame): DataFrame containing normalized protein abundance data.\n",
    "    MD (pd.DataFrame): DataFrame containing normalized mRNA abundance data.\n",
    "    rgb_tensor (torch.Tensor): The RGB tensor.\n",
    "    gene_name (str): The gene name.\n",
    "    label (int): The label.\n",
    "    numbering (bool): Whether to number the pixels. Default is True.\n",
    "    label_type (str): Type of labeling for the pixels. Options are 'order', 'abundance', 'index', 'none'.\n",
    "                      Default is 'order'.\n",
    "    \"\"\"\n",
    "    canvas = rgb_tensor.numpy().transpose(1, 2, 0)  # Convert to HWC format for plotting\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(canvas)\n",
    "\n",
    "    if numbering:\n",
    "        num_pixels = canvas.shape[0] * canvas.shape[1]\n",
    "        coords = gen_horizontal_coords(canvas.shape[0], num_pixels)\n",
    "\n",
    "        if label_type in ['abundance', 'index']:\n",
    "            final_array = interleave_arrays(set, PD, MD)\n",
    "            np.set_printoptions(suppress=True) # Suppress scientific notation and set decimal precision\n",
    "            final_array = final_array.flatten()\n",
    "            sum_product = PD + (MD if MD is not None else 0)\n",
    "            sorted_indices = np.argsort(sum_product, axis=1)\n",
    "            sorted_indices = sorted_indices.flatten()\n",
    "\n",
    "        for i, (x, y) in enumerate(coords):\n",
    "            if label_type == 'order':\n",
    "                label_text = str(i + 1)\n",
    "            elif label_type == 'abundance':\n",
    "                label_text = f'{final_array[i]:.2f}'\n",
    "            elif label_type == 'index':\n",
    "                original_index = sorted_indices[i // (2 if set == 'Protein + mRNA' else 1)]\n",
    "                label_text = str(original_index)\n",
    "            else:\n",
    "                raise ValueError(\"label_type must be 'order', 'abundance', or 'index'\")\n",
    "            ax.text(y, x, label_text, ha='center', va='center', color='white', fontsize=6)\n",
    "    \n",
    "    title_map = {\n",
    "        'order': \"Pixel plotting order\",\n",
    "        'abundance': \"Tumor Abundances of each pixel\",\n",
    "        'index': \"Tumor Indices of each pixel\",\n",
    "        'none': \"\"\n",
    "    }\n",
    "    ax.set_title(f\"Gene: {gene_name}, Label: {label.item()}, {title_map.get(label_type, '')}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f7580",
   "metadata": {},
   "source": [
    "### Transformation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTransform:\n",
    "    \"\"\"\n",
    "    A class that applies normal noise transformation to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        mean (float): The mean of the normal distribution. Default is 0.\n",
    "        std_dev (float): The standard deviation of the normal distribution. Default is 0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=0, std_dev=StandardDeviation):\n",
    "        super(NoiseTransform, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "\n",
    "    def forward(self, df):\n",
    "        \"\"\"\n",
    "        Applies normal noise transformation to the input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed DataFrame with normal noise applied.\n",
    "        \"\"\"\n",
    "        tensor = torch.from_numpy(df.values).float()\n",
    "        noise = torch.empty(tensor.size()).normal_(self.mean, self.std_dev)\n",
    "        noisy_tensor = tensor + noise\n",
    "        \n",
    "        # Convert the tensor back to DataFrame\n",
    "        df_transformed = pd.DataFrame(noisy_tensor.numpy(), index=df.index, columns=df.columns)\n",
    "        \n",
    "        return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTransform:\n",
    "    \"\"\"\n",
    "    A class representing a combined transformation.\n",
    "\n",
    "    Args:\n",
    "        set: The dataset used for the transformation.\n",
    "        transform1: The first transformation to apply.\n",
    "        transform2: The second transformation to apply (optional).\n",
    "        FeatureEngineering: Whether to use the feature engineering data.\n",
    "\n",
    "    Attributes:\n",
    "        transform1: The first transformation.\n",
    "        transform2: The second transformation.\n",
    "        FeatureEngineering: Whether to use the feature engineering data.\n",
    "\n",
    "    Methods:\n",
    "        __call__: Applies the combined transformation to the input dataframes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, set, transform1=None, transform2=None):\n",
    "        self.set = set\n",
    "        self.transform1 = transform1 if transform1 is not None else None\n",
    "        self.transform2 = transform2 if transform2 is not None else None\n",
    "\n",
    "    def __call__(self, set, df1, df2):\n",
    "        \"\"\"\n",
    "        Applies the combined transformation to the input dataframes.\n",
    "\n",
    "        Args:\n",
    "            set: The dataset used for the transformation.\n",
    "            df1: The first dataframe.\n",
    "            df2: The second dataframe.\n",
    "\n",
    "        Returns:\n",
    "            transformed_rgb_tensors: The transformed RGB tensors.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the first transformation (noise) to df1 and df2; df1 is PD, df2 is MD\n",
    "        if self.transform1 is not None:\n",
    "            df1 = self.transform1.forward(df1) if df1 is not None else None\n",
    "            df2 = self.transform1.forward(df2) if df2 is not None else None\n",
    "            \n",
    "        # apply the second transformation (random column) to df1 and df2\n",
    "        if self.transform2 is not None:\n",
    "            df1, df2 = self.transform2.forward(df1, df2) if df1 is not None and df2 is not None else (None, None)\n",
    "        \n",
    "        # if we are using feature engineering, derive third dataframe from already noise and/or column random transformed \n",
    "        if self.set == 'Protein':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, df1, None)\n",
    "        elif self.set == 'mRNA':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, None, df2)\n",
    "        elif self.set == 'Protein + mRNA':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, df1, df2)\n",
    "        \n",
    "        if transformed_rgb_tensors is None:\n",
    "            raise ValueError(\"Set must be 'Protein', 'mRNA', or 'Protein + mRNA'\")\n",
    "                \n",
    "        return transformed_rgb_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee71e5",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, set, PD, MD, labels, gene_names, transform=None):\n",
    "        self.set = set\n",
    "        self.PD = PD if PD is not None else None\n",
    "        self.MD = MD if MD is not None else None\n",
    "        self.labels = labels\n",
    "        self.gene_names = gene_names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pd_data = self.PD.iloc[idx].to_frame().T if self.PD is not None else None\n",
    "        md_data = self.MD.iloc[idx].to_frame().T if self.MD is not None else None\n",
    "        label = self.labels[idx]\n",
    "        gene_name = self.gene_names[idx]\n",
    "\n",
    "        if self.transform:  \n",
    "            if self.set == 'Protein + mRNA':\n",
    "                tensor = self.transform(self.set, pd_data, md_data)\n",
    "            elif self.set == 'Protein':\n",
    "                tensor = self.transform(self.set, pd_data, None)\n",
    "            elif self.set == 'mRNA':\n",
    "                tensor = self.transform(self.set, None, md_data)\n",
    "\n",
    "        else:\n",
    "            if self.set == 'Protein + mRNA':\n",
    "                tensor = create_rgb_tensors(self.set, pd_data, md_data)\n",
    "            elif self.set == 'Protein':\n",
    "                tensor = create_rgb_tensors(self.set, pd_data, None)\n",
    "            elif self.set == 'mRNA':\n",
    "                tensor = create_rgb_tensors(self.set, None, md_data)\n",
    "\n",
    "        # Ensure tensor is not None\n",
    "        if tensor is None:\n",
    "            raise ValueError(\"Both PD and MD are None, cannot create tensor.\")\n",
    "\n",
    "        return tensor, label, gene_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e719b39",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9cfd9",
   "metadata": {},
   "source": [
    "### Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transform\n",
    "noise_transform = NoiseTransform()\n",
    "\n",
    "# Create an instance of the combined transformï¼Œ change here if doing different transformation\n",
    "# Transform 1 default is noise_transform, transform 2 default is TBD\n",
    "transform = CombinedTransform(set=Set, transform1=noise_transform, transform2=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b651e",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4713f80-0717-4831-923e-4a2aa9949dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to ensure reproducibility\n",
    "set_seed(Seed)\n",
    "\n",
    "# Create a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label dataframe\n",
    "labels = encoder.fit_transform(LD.values.ravel())\n",
    "\n",
    "# Convert the labels to tensors\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Split the data into training and validation sets using the labels indices, random state is set\n",
    "indices = range(len(labels))\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(indices, labels, test_size=Validation_Fraction, random_state=42)\n",
    "\n",
    "# Use the indices to split PD and MD\n",
    "if Set == 'Protein' or 'Protein + mRNA':\n",
    "    train_PD = PD.iloc[train_indices]\n",
    "    val_PD = PD.iloc[val_indices]\n",
    "    print('Proteome data set is splited')\n",
    "\n",
    "if Set == 'mRNA' or 'Protein + mRNA':\n",
    "    train_MD = MD.iloc[train_indices]\n",
    "    val_MD = MD.iloc[val_indices]\n",
    "    print('mRNA data set is splited')\n",
    "\n",
    "# Save gene names\n",
    "train_gene_names = train_PD.index.tolist()\n",
    "val_gene_names = val_PD.index.to_list()\n",
    "\n",
    "## Create the datasets\n",
    "# Initialize transform to None\n",
    "transform_to_use = None\n",
    "\n",
    "# Check if transformations should be applied\n",
    "if TransformOrNot:\n",
    "    transform_to_use = transform # transform is defined above\n",
    "\n",
    "# Check Dataset argument to determine which dataset to use\n",
    "if Set == 'Protein':\n",
    "    train_dataset = MyDataset(Set, train_PD, None, train_labels, train_gene_names, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, val_PD, None, val_labels, val_gene_names, transform=None)\n",
    "elif Set == 'mRNA':\n",
    "    train_dataset = MyDataset(Set, None, train_MD, train_labels, train_gene_names, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, None, val_MD, val_labels, val_gene_names, transform=None)\n",
    "elif Set == 'Protein + mRNA':\n",
    "    train_dataset = MyDataset(Set, train_PD, train_MD, train_labels, train_gene_names, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, val_PD, val_MD, val_labels, val_gene_names, transform=None)\n",
    "else:\n",
    "    raise ValueError(\"Set must be 'Protein', 'mRNA', or 'Protein + mRNA'\")\n",
    "\n",
    "# Create the training data loaders, iteration of each index happens here\n",
    "if ImbalanceSampler:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "    print('Train dataloader is balanced by ImbalancedDatasetSampler')\n",
    "else:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "    print('Train dataloader is not balanced by ImbalancedDatasetSampler')\n",
    "\n",
    "# Create the validation data loader, shuffling is not necessary\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "print('Validation dataloader is not shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14169f0",
   "metadata": {},
   "source": [
    "### Check quantity of each label in dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba269809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the labels corresponding to each class\n",
    "for i, label in enumerate(encoder.classes_):\n",
    "    print(f\"{i} is {label}\")\n",
    "\n",
    "# Initialize a Counter object\n",
    "train_class_counts = Counter()\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for _, labels, _ in train_dataloader:\n",
    "    # Update the Counter with the labels in the current batch\n",
    "    train_class_counts.update(labels.numpy())\n",
    "\n",
    "# Print the class distribution\n",
    "print(f\"Training set label counts: {sorted(train_class_counts.items())}\")\n",
    "\n",
    "# Initialize a Counter object\n",
    "val_class_counts = Counter()\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for _, labels, _ in val_dataloader:\n",
    "    # Update the Counter with the labels in the current batch\n",
    "    val_class_counts.update(labels.numpy())\n",
    "\n",
    "# Print the class distribution\n",
    "print(f\"Validation set label counts: {sorted(val_class_counts.items())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6729a",
   "metadata": {},
   "source": [
    "### Visualize the tensors in data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train data loader to see the transformed tensors\n",
    "train_batch_images, train_batch_labels, train_batch_gene_names = next(iter(train_dataloader))\n",
    "\n",
    "# Use first image in the batch\n",
    "first_image_in_train_batch = train_batch_images[0]\n",
    "image_to_plot = first_image_in_train_batch.permute(1, 2, 0).numpy().astype('uint8')\n",
    "first_gene_in_train_batch = train_batch_gene_names[0]\n",
    "first_label_in_train_batch = train_batch_labels[0]\n",
    "\n",
    "if isinstance(train_PD.loc[first_gene_in_train_batch], pd.DataFrame) and isinstance(train_MD.loc[first_gene_in_train_batch], pd.DataFrame):\n",
    "    # If it's a DataFrame, take the first row\n",
    "    PD_to_plot = train_PD.loc[first_gene_in_train_batch][:1].T\n",
    "    MD_to_plot = train_MD.loc[first_gene_in_train_batch][:1].T\n",
    "else:\n",
    "     # If it's a Series, convert to DataFrame with a single row if needed\n",
    "    PD_to_plot = train_PD.loc[first_gene_in_train_batch].to_frame().T\n",
    "    MD_to_plot = train_MD.loc[first_gene_in_train_batch].to_frame().T\n",
    "\n",
    "\n",
    "# Visualize the tensor as an image using PlotCanvasWithNumbers\n",
    "if Set == 'Protein + mRNA':\n",
    "    annotate_canvas(Set, PD_to_plot, MD_to_plot, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, PD_to_plot, MD_to_plot, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='index')\n",
    "elif Set == 'Protein':\n",
    "    annotate_canvas(Set, PD_to_plot, None, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, PD_to_plot, None, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='index')\n",
    "\n",
    "elif Set == 'mRNA':\n",
    "    annotate_canvas(Set, None, MD_to_plot, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, None, MD_to_plot, first_image_in_train_batch, first_gene_in_train_batch, first_label_in_train_batch, numbering=True, label_type='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3506ba-ba1e-49ff-ab81-7ef38ff02333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train data loader to see the transformed tensors\n",
    "val_batch_images, val_batch_labels, val_batch_gene_names = next(iter(val_dataloader))\n",
    "\n",
    "# Use first image in the batch\n",
    "first_image_in_val_batch = val_batch_images[0]\n",
    "image_to_plot = first_image_in_val_batch.permute(1, 2, 0).numpy().astype('uint8')\n",
    "first_gene_in_val_batch = val_batch_gene_names[0]\n",
    "first_label_in_val_batch = val_batch_labels[0]\n",
    "\n",
    "# Visualize the tensor as an image using PlotCanvasWithNumbers\n",
    "if Set == 'Protein + mRNA':\n",
    "    annotate_canvas(Set, val_PD.loc[first_gene_in_val_batch].to_frame().T, val_MD.loc[first_gene_in_val_batch].to_frame().T, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, val_PD.loc[first_gene_in_val_batch].to_frame().T, val_MD.loc[first_gene_in_val_batch].to_frame().T, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='index')\n",
    "elif Set == 'Protein': \n",
    "    annotate_canvas(Set, val_PD.loc[first_gene_in_val_batch].to_frame().T, None, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, val_PD.loc[first_gene_in_val_batch].to_frame().T, None, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='index')\n",
    "elif Set == 'mRNA':\n",
    "    annotate_canvas(Set, None, val_MD.loc[first_gene_in_val_batch].to_frame().T, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='abundance')\n",
    "    annotate_canvas(Set, None, val_MD.loc[first_gene_in_val_batch].to_frame().T, first_image_in_val_batch, first_gene_in_val_batch, first_label_in_val_batch, numbering=True, label_type='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3278b",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For recording the time and double check the sets before running\n",
    "print(Time_Stamp)\n",
    "print(f'Dataset: {Set}')\n",
    "print(f'Image size: {Canvas_Size} x {Canvas_Size}')\n",
    "print(f'PD: {len(PD.columns) if PD is not None else None}, MD: {len(MD.columns) if MD is not None else None}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321926f0",
   "metadata": {},
   "source": [
    "### CustomNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "\n",
    "        self.dropout_percentage = 0.4  # Reduced dropout\n",
    "\n",
    "        # Initial layer: 3x3 kernel, stride 1 (input 18x18 -> output 16x16)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv2: 3x3 kernel, stride 1 (output 14x14)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Max Pooling added after conv2 (output 7x7)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv3: 3x3 kernel, stride 1 (output 5x5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Max Pooling added after conv3 (output 2x2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv4: 2x2 kernel, stride 2 (output 1x1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Conv5: 1x1 kernel, stride 1 (output 1x1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        self.dropout6 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.dropout7 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # Adaptive Average Pooling (output 1x1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Smaller fully connected layer (512 units instead of 1000)\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "        self.fc_dropout = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.out = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: 64 channels, 16x16\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: 128 channels, 14x14\n",
    "        x = self.pool2(x)  # Max pooling (output 7x7)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: 256 channels, 5x5\n",
    "        x = self.pool3(x)  # Max pooling (output 2x2)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # Output: 512 channels, 1x1\n",
    "        x = F.relu(self.bn5(self.conv5(x)))  # Output: 1024 channels, 1x1\n",
    "\n",
    "        x = F.relu(self.bn6(self.conv6(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        x = F.relu(self.bn7(self.conv7(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc_dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with initial stride of 2\n",
    "class SimpleResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet2, self).__init__()\n",
    "\n",
    "        # Dropout percentage for the model\n",
    "        self.dropout_percentage = 0.5\n",
    "\n",
    "        # Conv1: 2x2 kernel, stride 2 (input 18x18 -> output 9x9)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv2: 2x2 kernel, stride 1 (output 8x8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Conv3: 2x2 kernel, stride 1 (output 7x7)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)  # Dropout after conv3\n",
    "\n",
    "        # Conv4: 2x2 kernel, stride 2 (output 3x3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout(p=0.3)  # Dropout after conv4\n",
    "\n",
    "        # Conv5: 3x3 kernel, stride 1 (output 1x1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "        self.dropout5 = nn.Dropout(p=0.4)  # Dropout after conv5\n",
    "\n",
    "        # Conv6: 2x2 kernel, stride 1 (remains 1x1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        self.dropout6 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv6\n",
    "\n",
    "        # Conv7: 2x2 kernel, stride 2 (remains 1x1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.dropout7 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv7\n",
    "\n",
    "        # Adaptive Average Pooling (output 1x1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(512, 1000)\n",
    "        self.fc_dropout = nn.Dropout(p=self.dropout_percentage)  # Dropout after fully connected layer\n",
    "        self.out = nn.Linear(1000, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: 64 channels, 9x9\n",
    "\n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: 128 channels, 8x8\n",
    "\n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: 256 channels, 7x7\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Fourth conv block with downsampling\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # Output: 512 channels, 3x3\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth conv block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))  # Output: 1024 channels, 1x1\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # Sixth conv block with dropout\n",
    "        x = F.relu(self.bn6(self.conv6(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        # Seventh conv block with dropout\n",
    "        x = F.relu(self.bn7(self.conv7(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        # Global Average Pooling to reduce size to 1x1\n",
    "        x = self.avgpool(x)  # Output: 512 channels, 1x1\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc_dropout(x)  # Dropout after fully connected layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bdd3d-a3e9-4e7b-993d-2639c86df6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with initial stride of 2 - Nate's\n",
    "class SimpleResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet2, self).__init__()\n",
    "\n",
    "        # Dropout percentage for the model\n",
    "        self.dropout_percentage = 0.5\n",
    "\n",
    "        # Conv1: 2x2 kernel, stride 2 (input 18x18 -> output 9x9)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Conv2: 2x2 kernel, stride 1 (output 8x8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv3: 2x2 kernel, stride 1 (output 7x7)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)  # Dropout after conv3\n",
    "\n",
    "        # Conv4: 2x2 kernel, stride 2 (output 6x6)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(8)\n",
    "        self.dropout4 = nn.Dropout(p=0.3)  # Dropout after conv4\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(in_features=6*6*8, out_features=128)\n",
    "        self.fc_dropout = nn.Dropout(p=self.dropout_percentage)  # Dropout after fully connected layer\n",
    "        self.out = nn.Linear(128, 4)\n",
    "\n",
    "    # initialize with random weights\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: 64 channels, 9x9\n",
    "\n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: 128 channels, 8x8\n",
    "\n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: 256 channels, 7x7\n",
    "        # x = self.dropout3(x)\n",
    "\n",
    "        # Fourth conv block with downsampling\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # Output: 512 channels, 3x3\n",
    "        # x = self.dropout4(x)\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc(x))\n",
    "        # x = self.fc_dropout(x)  # Dropout after fully connected layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109d1f6",
   "metadata": {},
   "source": [
    "### Training and testing epoch loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over each \"batch\" stored in the DataLoader object (dataloader)\n",
    "    # Unpack each batch into the tuple (X,y,_), where X is the array of images in the batch and y the array of labels. \n",
    "    #   \"_\" allows for anything else to be unpacked if it's present\n",
    "    #   A batch in the DataLoader is a tuple\n",
    "    for batch, (X, y, _) in enumerate(dataloader):\n",
    "\n",
    "        # Prepare data\n",
    "        X = X.to(torch.float32) # convert to float32 to avoid error stating byte expected but found float\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass: predict classes\n",
    "        # calling a nn.Module object with an input will lead to the forward() method being called\n",
    "        # in our case the SimpleResNet2 object is a child of the nn.Module() object as specified in the SimpleResNet2 class definition\n",
    "        # an array of predicted labels for each image in X is returned\n",
    "        pred = model(X)\n",
    "    \n",
    "        # Compute the cross-entropy loss \n",
    "        # The loss is computed based on the array of predicted labels, their probabilities, and the actual labels\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = F.softmax(pred, dim=1)\n",
    "\n",
    "        # Compute the entropy of the softmax output for the confidence penalty\n",
    "        entropy = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-10), dim=1)\n",
    "\n",
    "        # Confidence penalty: encourage high entropy (i.e., reduce overconfidence)\n",
    "        penalty = torch.mean(entropy)\n",
    "        \n",
    "        # Total loss: cross-entropy loss + confidence penalty (scaled by lambda); lambda set to 0.1\n",
    "        total_loss_with_penalty = loss - 0.1 * penalty\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # zero the parameter gradients \n",
    "        total_loss_with_penalty.backward() # backpropagate the loss\n",
    "        optimizer.step() # adjust parameters based on the calculated gradients\n",
    "\n",
    "        total_loss += loss.item() # extract the loss value\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if LrScheduler:\n",
    "        exp_lr_scheduler.step()   \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, current_lr\n",
    "            \n",
    "# Function for validating the model\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    \n",
    "    for X, y, _ in dataloader:\n",
    "        \n",
    "        # Prepare data\n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass: predict classes\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = F.softmax(pred, dim=1)\n",
    "\n",
    "        # Compute the loss\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        # Calculate accuracy: choose the class with the highest probability\n",
    "        accuracy += (softmax_probs.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "    test_loss /= num_batches\n",
    "    accuracy /= size\n",
    "        \n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Function for testing the model with confusion matrix, and softmax output with a condition of >50% certanty\n",
    "def test_loop_with_confusion_matrix(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, accuracy, no_prediction_count = 0, 0, 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for X, y, _ in dataloader:\n",
    "        # convert to float32 to avoid byte/float mismatch error\n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get the maximum probability and its corresponding class\n",
    "        max_prob, predicted_class = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        # Only consider predictions with a probability greater than 0.5\n",
    "        accepted_probability = 0\n",
    "        valid_predictions = max_prob > accepted_probability\n",
    "        \n",
    "        # Convert the labels to the same device and calculate the loss\n",
    "        y = y.to(device)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        \n",
    "        # Append valid predictions and corresponding labels to lists for confusion matrix\n",
    "        all_preds.extend(predicted_class[valid_predictions].cpu().numpy())\n",
    "        all_labels.extend(y[valid_predictions].cpu().numpy())\n",
    "        \n",
    "        # Count valid predictions where probability is greater than 0.5\n",
    "        valid_preds_count = valid_predictions.sum().item()\n",
    "        \n",
    "        # Calculate accuracy only for valid predictions\n",
    "        accuracy += (predicted_class[valid_predictions] == y[valid_predictions]).type(torch.float).sum().item()\n",
    "        \n",
    "        # Count how many predictions were skipped (i.e., where no probability exceeded 0.5)\n",
    "        no_prediction_count += (valid_predictions == 0).sum().item()\n",
    "\n",
    "        # Calculate the precision and recall\n",
    "        precision = precision_score(all_labels, all_preds, average=None)\n",
    "        recall = recall_score(all_labels, all_preds, average=None)\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "\n",
    "\n",
    "    accuracy /= (size - no_prediction_count)  # Normalize accuracy by valid predictions\n",
    "    \n",
    "    no_prediction_percentage = no_prediction_count / size * 100\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Skipped predictions (no class > 50%): {no_prediction_percentage:.2f}%\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    return test_loss, accuracy, conf_matrix, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0566c",
   "metadata": {},
   "source": [
    "### Confusion matrix, precision and recall calculation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the precision and recall\n",
    "def calculate_precision_recall(dataloader, model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over validation data\n",
    "    for X, y, _ in dataloader:\n",
    "        \n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        output = model(X)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = y.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate precision and recall for each class\n",
    "    precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    return cf_matrix, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d1232",
   "metadata": {},
   "source": [
    "### F1 score helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59556bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_metrics(conf_matrix):\n",
    "    # True positives are the diagonal elements\n",
    "    tp = np.diag(conf_matrix)\n",
    "    # False positives are the sum of each column minus the diagonal element\n",
    "    fp = np.sum(conf_matrix, axis=0) - tp\n",
    "    # False negatives are the sum of each row minus the diagonal element\n",
    "    fn = np.sum(conf_matrix, axis=1) - tp\n",
    "    # True negatives are the sum of all elements minus the sum of the corresponding row and column plus tp\n",
    "    tn = conf_matrix.sum() - (fp + fn + tp)\n",
    "    \n",
    "    # Precision, recall, and F1 score calculations\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Return macro-averaged F1 score (excluding NaNs caused by divisions by zero)\n",
    "    return np.nanmean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e94c70",
   "metadata": {},
   "source": [
    "### Define early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0001, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in validation loss to qualify as an improvement.\n",
    "            verbose (bool): If True, prints a message for each improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset the counter if there's an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bdc13",
   "metadata": {},
   "source": [
    "### Define loss functions and class weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d88c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        :param gamma: focusing parameter (default=2.0)\n",
    "        :param alpha: balance parameter, it can be a float or a tensor (default=None)\n",
    "        :param reduction: specify the reduction to apply to the output: 'none' | 'mean' | 'sum' (default='mean')\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute cross-entropy loss\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # pt is the probability of the true class\n",
    "        \n",
    "        # Compute the focal loss\n",
    "        F_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        # Apply class weighting (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha[targets]\n",
    "            F_loss = alpha * F_loss\n",
    "        \n",
    "        # Apply the specified reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5baf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# https://medium.com/@zergtant/use-weighted-loss-function-to-solve-imbalanced-data-classification-problems-749237f38b75\n",
    "if ApplyClassWeightToLoss:\n",
    "    # Weights for each class for loss function\n",
    "    LossWeights = torch.tensor([0.8341,2.6591,1.2474,1.5630]) # calculate by log inverse class frequency np.log(total/(# + 1)), it's better\n",
    "    # LossWeights = torch.tensor([2.3042, 14.3370, 3.4842, 4.7790]) # nate's weight (torch.tensor([n_cytosol,n_mitochondria,n_nuclear,n_secretory])/n_total)**-1\n",
    "    # LossWeights = torch.tensor([0.569, 3.571, 0.864, 1.241]) # inverse class frequency weight wj=N/(nj*4)\n",
    "    # LossWeights = torch.tensor([0.569, 3.559, 0.864, 1.239]) # inverse class frequency weight smoothed wj=(N+smooth)/(nj+smooth*4)\n",
    "    # LossWeights = torch.tensor([1.000, 2.836, 1.417, 1.779]) # log scaling wj=1+(log(max(majority class))/nj)\n",
    "    # LossWeights = torch.tensor([1.139, 7.142, 1.728, 2.481]) # balanced weight wj = N/(nj*2)\n",
    "    \n",
    "    LossWeights = LossWeights.to(device)\n",
    "    \n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing, weight=LossWeights)\n",
    "\n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=LossWeights, reduction='mean')\n",
    "\n",
    "else:\n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing)\n",
    "    \n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e79a8",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "if Model == 'ResNet18': # Resnet18\n",
    "    model = models.resnet18(weights=None) # without initial weights defined - good for patients not re-arranged\n",
    "    model.fc = nn.Sequential(\n",
    "        model.fc,\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1000, 4))\n",
    "\n",
    "elif Model == 'CustomNN-reg':\n",
    "    model = SimpleResNet() # for 18x18\n",
    "\n",
    "elif Model == 'CustomNN-stride2':\n",
    "    recall = np.array([0, 0, 0, 0])\n",
    "    precision = np.array([0, 0, 0, 0])\n",
    "    i = 0\n",
    "    while sum((recall < 0.35) & (recall > 0.10))!=4:\n",
    "        model = SimpleResNet2() # for 18x18 with initial stride 2\n",
    "        model = model.to(dev)\n",
    "        val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "        #print(precision) #cyt mito nuc sec\n",
    "        if i%50 == 0:\n",
    "            print(i)\n",
    "            print(recall)\n",
    "        i = i+1\n",
    "    print('done!')\n",
    "        \n",
    "\n",
    "elif Model == 'CustomNN-mini':\n",
    "    model = SimpleResNet_Mini() # for 12x12\n",
    "\n",
    "# i need to move the model to the GPU to evaluate precision and recall in the loop where it is created above\n",
    "#model = model.to(dev)\n",
    "\n",
    "# see the model architecture if desired\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def Optimizer(name, model):\n",
    "    if name == 'SGD_weight':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay, momentum=Momentum)\n",
    "    if name == 'SGD_default':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate)\n",
    "    if name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay)\n",
    "        \n",
    "    return optimizer, name\n",
    "\n",
    "# call optimizer\n",
    "optimizer, optimizer_name = Optimizer('Adam', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1cd7",
   "metadata": {},
   "source": [
    "### Define the learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LR scheduler\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Define the cosine annealing scheduler\n",
    "# exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-5)\n",
    "\n",
    "# Define the cosine annealing warm restarts scheduler\n",
    "#exp_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=60, T_mult=2, eta_min=0.01*Learner_rate) # 1% of initial learning rate as minimum\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.10*Learner_rate) # 1% of initial learning rate as minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f4119-2a9d-4c51-9242-ecc2e14d72bb",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 4000\n",
    "early_stop_patience = epochs\n",
    "\n",
    "\n",
    "# Define the model\n",
    "recall = np.array([0, 0, 0, 0])\n",
    "precision = np.array([0, 0, 0, 0])\n",
    "i = 0\n",
    "while sum((recall < 0.35) & (recall > 0.15))!=4:\n",
    "    model = SimpleResNet2() # for 18x18 with initial stride 2\n",
    "    model = model.to(dev)\n",
    "    val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "    #print(precision) #cyt mito nuc sec\n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "        print(recall) #cyt mito nuc sec\n",
    "    i = i+1\n",
    "print('Initial model weights determined')\n",
    "print('recall:')\n",
    "print(recall)\n",
    "print('precision:')\n",
    "print(precision)\n",
    "\n",
    "\n",
    "# Define Optimizer and Learner Rate Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay)\n",
    "optimizer_name = 'Adam'\n",
    "# Define the cosine annealing warm restarts scheduler\n",
    "#exp_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=60, T_mult=2, eta_min=0.01*Learner_rate) # 1% of initial learning rate as minimum\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=2, eta_min=0.10*Learner_rate) # 1% of initial learning rate as minimum\n",
    "\n",
    "# Run the Model\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "# set_seed(Seed)\n",
    "\n",
    "# Set up performance dataframe to record loss and accuracy of each epoch\n",
    "PerformanceDF = pd.DataFrame(index=range(epochs),columns=['Epoch','TrainLoss','ValLoss','TrainAcc','ValAcc'])\n",
    "print(f\"{'Epoch':>5s}{'Train Loss':>13s}{'Val Loss':>11s}{'Train Accuracy':>17s}{'Val Accuracy':>15s}{'Current LR':>13s}\\n\")\n",
    "PrecisionRecallDF = pd.DataFrame(columns=['Epoch', 'Class', 'Precision', 'Recall'])\n",
    "index = 0\n",
    "current_lr = Learner_rate\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=early_stop_patience, min_delta=0.0001, verbose=True)\n",
    "\n",
    "# epochs\n",
    "for t in range(epochs+1):\n",
    "\n",
    "    if t == 0:\n",
    "        # get initial model losses, accuracies from both training and validation sets\n",
    "        train_loss, train_accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "        val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
    "        \n",
    "        # get initial model precision and recall for each class\n",
    "        val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "\n",
    "    else:\n",
    "        # training the model, output the losses, get the training and validation accuracy on the fly\n",
    "        model.train()\n",
    "        train_loss, current_lr = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        _, train_accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
    "            val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "        \n",
    "    Epoch = t\n",
    "    print(f\"{str(Epoch):>5s}{train_loss:>13f}{val_loss:>11f}{train_accuracy:>16f}{val_accuracy:>14f}{current_lr:>16f}\")\n",
    "    PerformanceDF.loc[t,:]=[Epoch,train_loss,val_loss,train_accuracy,val_accuracy]\n",
    "    for i in range(4):\n",
    "        PrecisionRecallDF.loc[index]=[Epoch, i, precision[i], recall[i]]\n",
    "        index += 1\n",
    "    \n",
    "    if t == 0:\n",
    "        highest_val_accuracy = 0 \n",
    "        MSD0 = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "        MSD_Best = copy.deepcopy(model.state_dict())\n",
    "        # Record the epoch when the best model is saved\n",
    "        Best_Epoch = t\n",
    "        torch.save({\n",
    "            'model_state_dict': MSD_Best,\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            Path(model_folder_path + f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "\n",
    "    if t == epochs:\n",
    "        MSD_Final = copy.deepcopy(model.state_dict())\n",
    "        torch.save({\n",
    "            'model_state_dict': MSD_Final,\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            Path(model_folder_path + f'last_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    \n",
    "    # Check for early stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break  # Stop the training loop if early stopping is triggered\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6279727",
   "metadata": {},
   "source": [
    "## Testing set prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca84d2",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27652e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the synthetic data\n",
    "if Set == 'Protein + mRNA':\n",
    "    # Johansson\n",
    "    J_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_prot+mRNA_PD_synthetic.csv'))\n",
    "    J_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_prot+mRNA_MD_synthetic.csv'))\n",
    "    \n",
    "    # Mertins\n",
    "    M_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_prot+mRNA_PD_synthetic.csv'))\n",
    "    M_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_prot+mRNA_MD_synthetic.csv'))\n",
    "\n",
    "    # Wrangling\n",
    "    J_PD_synthetic.index = J_PD_synthetic.loc[:,'Unnamed: 0']\n",
    "    J_MD_synthetic.index = J_MD_synthetic.loc[:,'Unnamed: 0']\n",
    "    M_PD_synthetic.index = M_PD_synthetic.loc[:,'Unnamed: 0']\n",
    "    M_MD_synthetic.index = M_MD_synthetic.loc[:,'Unnamed: 0']\n",
    "\n",
    "    J_PD_synthetic = J_PD_synthetic.loc[:, J_PD_synthetic.columns!='Unnamed: 0']\n",
    "    J_MD_synthetic = J_MD_synthetic.loc[:, J_MD_synthetic.columns!='Unnamed: 0']\n",
    "    M_PD_synthetic = M_PD_synthetic.loc[:, M_PD_synthetic.columns!='Unnamed: 0']\n",
    "    M_MD_synthetic = M_MD_synthetic.loc[:, M_MD_synthetic.columns!='Unnamed: 0']\n",
    "\n",
    "elif Set == 'Protein':\n",
    "    # Johansson\n",
    "    J_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_prot_PD_synthetic.csv'))\n",
    "    J_PD_synthetic.index = J_PD_synthetic.loc[:,'Unnamed: 0']\n",
    "    J_PD_synthetic = J_PD_synthetic.loc[:, J_PD_synthetic.columns!='Unnamed: 0']\n",
    "\n",
    "    # Mertins\n",
    "    M_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_prot_PD_synthetic.csv'))\n",
    "    M_PD_synthetic.index = M_PD_synthetic.loc[:,'Unnamed: 0']\n",
    "    M_PD_synthetic = M_PD_synthetic.loc[:, M_PD_synthetic.columns!='Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cff554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the label\n",
    "J_LD = pd.read_csv(Path(dataset_folder_path+'Johansson_Localization.csv')) # gene labels that are mutual in proteome and transcriptome\n",
    "M_LD = pd.read_csv(Path(dataset_folder_path+'Mertins_Localization.csv')) # gene labels that are mutual in proteome and transcriptome\n",
    "\n",
    "# Data set wrangling\n",
    "J_LD.index = J_LD.loc[:,'Protein']\n",
    "J_LD = J_LD.loc[:,J_LD.columns!='Protein']\n",
    "M_LD.index = M_LD.loc[:,'Protein']\n",
    "M_LD = M_LD.loc[:,M_LD.columns!='Protein']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "#set_seed(Seed)\n",
    "\n",
    "# Create a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label df\n",
    "J_labels = encoder.fit_transform(J_LD.values.ravel())\n",
    "M_labels = encoder.fit_transform(M_LD.values.ravel())\n",
    "\n",
    "# Convert the labels to tensors\n",
    "J_labels = torch.tensor(J_labels)\n",
    "M_labels = torch.tensor(M_labels)\n",
    "\n",
    "# Save gene names\n",
    "J_test_gene_names = J_PD_synthetic.index.to_list()\n",
    "M_test_gene_names = M_PD_synthetic.index.to_list()\n",
    "\n",
    "# Check Dataset argument to determine which dataset to use\n",
    "if Set == 'Protein':\n",
    "    J_test_dataset = MyDataset(Set, J_PD_synthetic, None, J_labels, J_test_gene_names, transform=None)\n",
    "    M_test_dataset = MyDataset(Set, M_PD_synthetic, None, M_labels, M_test_gene_names, transform=None)\n",
    "    print('Protein dataset loaded')\n",
    "elif Set == 'Protein + mRNA':\n",
    "    J_test_dataset = MyDataset(Set, J_PD_synthetic, J_MD_synthetic, J_labels, J_test_gene_names, transform=None)\n",
    "    M_test_dataset = MyDataset(Set, M_PD_synthetic, M_MD_synthetic, M_labels, M_test_gene_names, transform=None)\n",
    "    print('Protein + mRNA dataset loaded')\n",
    "else:\n",
    "    raise ValueError(\"Set must be 'Protein', or 'Protein + mRNA'\")\n",
    "\n",
    "# Create the dataloaders\n",
    "J_test_dataloader = DataLoader(J_test_dataset, batch_size=batch, shuffle=False)\n",
    "M_test_dataloader = DataLoader(M_test_dataset, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27510fc4",
   "metadata": {},
   "source": [
    "### Re-define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe68a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "if ApplyClassWeightToLoss:\n",
    "    # Weights for each class for loss function\n",
    "    LossWeights = torch.tensor([0.8341,2.6591,1.2474,1.5630]) # calculate by log inverse class frequency np.log(total/(# + 1)), it's better\n",
    "    # LossWeights = torch.tensor([2.3042, 14.3370, 3.4842, 4.7790]) # nate's weight (torch.tensor([n_cytosol,n_mitochondria,n_nuclear,n_secretory])/n_total)**-1\n",
    "    # LossWeights = torch.tensor([0.569, 3.571, 0.864, 1.241]) # inverse class frequency weight wj=N/(nj*4)\n",
    "    # LossWeights = torch.tensor([0.569, 3.559, 0.864, 1.239]) # inverse class frequency weight smoothed wj=(N+smooth)/(nj+smooth*4)\n",
    "    # LossWeights = torch.tensor([1.000, 2.836, 1.417, 1.779]) # log scaling wj=1+(log(max(majority class))/nj)\n",
    "    # LossWeights = torch.tensor([1.139, 7.142, 1.728, 2.481]) # balanced weight wj = N/(nj*2)\n",
    "    # LossWeights = torch.tensor([1, 3, 1.5, 2])\n",
    "    \n",
    "    LossWeights = LossWeights.to(device)\n",
    "    \n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing, weight=LossWeights)\n",
    "\n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=LossWeights, reduction='mean')\n",
    "\n",
    "else:\n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing)\n",
    "    \n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae924a30",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384f744-1e3c-4c19-8b53-89d88024a4b8",
   "metadata": {},
   "source": [
    "### Fig 1: training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573797e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph loss for training and validation\n",
    "figure1 = plt.gcf()\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['TrainLoss'])\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['ValLoss'])\n",
    "plt.xlim([0,epochs])\n",
    "plt.ylim([0, 1.8])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1232ff6",
   "metadata": {},
   "source": [
    "### Fig 2: training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4beabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best model\n",
    "if Set == 'Protein':\n",
    "    best_model = SimpleResNet()\n",
    "    checkpoint = torch.load(Path(model_folder_path+f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "elif Set == 'Protein + mRNA':\n",
    "    best_model = SimpleResNet2()\n",
    "    checkpoint = torch.load(Path(model_folder_path + f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "best_model.to(device)\n",
    "\n",
    "# Evalulate validation set (accuracy, F1, confusion matrix)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Accuracy\n",
    "    val_loss, val_accuracy = test_loop(val_dataloader, best_model, loss_fn)\n",
    "    print(f\"Validation accuracy by best model: {round(val_accuracy, 3)}\")\n",
    "    \n",
    "    # Confusion matrix, precision, recall\n",
    "    val_cf_matrix, val_precision, val_recall = calculate_precision_recall(val_dataloader, best_model)\n",
    "    print(f'Validation precision by best model: Cytosol {round(val_precision[0],3)}, Mitochondrial {round(val_precision[1],3)}, Nuclear {round(val_precision[2],3)}, Secretory {round(val_precision[3],3)}')\n",
    "    print(f'Validation recall by best model: Cytosol {round(val_recall[0],3)}, Mitochondrial {round(val_recall[1],3)}, Nuclear {round(val_recall[2],3)}, Secretory {round(val_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    val_f1 = calculate_f1_metrics(val_cf_matrix)\n",
    "    print(f'Validation F1 score by best model is {round(val_f1, 3)}')\n",
    "\n",
    "# Confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=val_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "figure3 = plt.savefig(Path(figure_folder_path+f'figure3_val_cm_best_model_{Run_Name}_{Time_Stamp}.png'))\n",
    "#figure3 = plt.savefig('/home/ec2-user/MLNotebook/Figure exports/'+f'figure3_val_cm_best_model_{Run_Name}_{Time_Stamp}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph accuracy for training and validation\n",
    "figure2 = plt.gcf()\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['TrainAcc'])\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['ValAcc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim([0,epochs])\n",
    "plt.ylim([0,1.1])\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ca104",
   "metadata": {},
   "source": [
    "### Fig 3: validation confusion matrix by best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9132813",
   "metadata": {},
   "source": [
    "### Fig 4: testing confusion matrics by best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalulate test set (accuracy, precision, recall, F1)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Johansson test set\n",
    "    # Accuracy, precision, recall\n",
    "    J_test_loss, J_test_accuracy, J_test_cf_matrix, J_precision, J_recall = test_loop_with_confusion_matrix(J_test_dataloader, best_model, loss_fn)\n",
    "    print(f\"Johansson testing accuracy by best model: {round(J_test_accuracy, 3)}\")\n",
    "    print(f'Johansson testing precision by best model: Cytosol {round(J_precision[0],3)}, Mitochondrial {round(J_precision[1],3)}, Nuclear {round(J_precision[2],3)}, Secretory {round(J_precision[3],3)}')\n",
    "    print(f'Johansson testing recall by best model: Cytosol {round(J_recall[0],3)}, Mitochondrial {round(J_recall[1],3)}, Nuclear {round(J_recall[2],3)}, Secretory {round(J_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    J_test_f1 = calculate_f1_metrics(J_test_cf_matrix)\n",
    "    print(f'Johansson testing F1 score by best model is {round(J_test_f1, 3)}')\n",
    "\n",
    "    # Mertins test set\n",
    "    # Accuracy, precision, recall\n",
    "    M_test_loss, M_test_accuracy, M_test_cf_matrix, M_precision, M_recall = test_loop_with_confusion_matrix(M_test_dataloader, best_model, loss_fn)\n",
    "    print(f\"Mertins testing accuracy by best model: {round(M_test_accuracy, 3)}\")\n",
    "    print(f'Mertins testing precision by best model: Cytosol {round(M_precision[0],3)}, Mitochondrial {round(M_precision[1],3)}, Nuclear {round(M_precision[2],3)}, Secretory {round(M_precision[3],3)}')\n",
    "    print(f'Mertins testing recall by best model: Cytosol {round(M_recall[0],3)}, Mitochondrial {round(M_recall[1],3)}, Nuclear {round(M_recall[2],3)}, Secretory {round(M_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    M_test_f1 = calculate_f1_metrics(M_test_cf_matrix)\n",
    "    print(f'Mertins testing F1 score by best model is {round(M_test_f1, 3)}')\n",
    "\n",
    "# Display the confusion matrices side by side\n",
    "disp_J = ConfusionMatrixDisplay(confusion_matrix=J_test_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "disp_M = ConfusionMatrixDisplay(confusion_matrix=M_test_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "disp_J.plot(cmap=plt.cm.Blues)\n",
    "disp_M.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576742b",
   "metadata": {},
   "source": [
    "### Fig 5: Precision and recall of validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cdcc18",
   "metadata": {},
   "source": [
    "#### 95% CI simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324daec4-3944-4d7f-8892-6d4ce75af396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and Recall Random Guess Simulations - Nate\n",
    "\n",
    "Validation_Class_Dist = sorted(val_class_counts.items()) #cyto mito, nuc, sec\n",
    "Val_ClassQuants = [Validation_Class_Dist[i][1] for i in np.arange(0,len(Validation_Class_Dist))]\n",
    "N_ValSet = sum(Val_ClassQuants)\n",
    "Val_ClassFracs = [i/sum(Val_ClassQuants) for i in Val_ClassQuants]\n",
    "Val_ClassFracs\n",
    "\n",
    "n_cyt = Val_ClassQuants[0]\n",
    "n_mito = Val_ClassQuants[1]\n",
    "n_nuc = Val_ClassQuants[2]\n",
    "n_sec = Val_ClassQuants[3]\n",
    "\n",
    "cyt_indices = [0,n_cyt]\n",
    "mito_indices = [n_cyt,n_cyt+n_mito]\n",
    "nuc_indices = [n_cyt+n_mito,n_cyt+n_mito+n_nuc]\n",
    "sec_indices = [n_cyt+n_mito+n_nuc,n_cyt+n_mito+n_nuc+n_sec]\n",
    "\n",
    "\n",
    "n_iterations = 10000\n",
    "recall_matrix = np.empty((n_iterations,4))\n",
    "precision_matrix = np.empty((n_iterations,4))\n",
    "classification_list = [None]*N_ValSet\n",
    "for iteration in np.arange(0,n_iterations):\n",
    "    for i in np.arange(0,N_ValSet):\n",
    "        Random_Number = random.random()\n",
    "        if Random_Number < 0.25:\n",
    "            classification_list[i] = 'cytosolic'\n",
    "        elif (Random_Number >= 0.25) & (Random_Number < 0.5):\n",
    "            classification_list[i] = 'mitochondrial'\n",
    "        elif (Random_Number >= 0.5) & (Random_Number < 0.75):\n",
    "            classification_list[i] = 'nuclear'\n",
    "        else:\n",
    "            classification_list[i] = 'secretory'\n",
    "    \n",
    "    # Lists of actually cytosolic, mitochondrial, nuclear, and secretory positions\n",
    "    cyt_list = classification_list[cyt_indices[0]:cyt_indices[1]]\n",
    "    mito_list = classification_list[mito_indices[0]:mito_indices[1]]\n",
    "    nuc_list = classification_list[nuc_indices[0]:nuc_indices[1]]\n",
    "    sec_list = classification_list[sec_indices[0]:sec_indices[1]]\n",
    "\n",
    "    # recall values for each compartment\n",
    "    cyt_recall = sum([p=='cytosolic' for p in cyt_list])/n_cyt\n",
    "    mito_recall = sum([p=='mitochondrial' for p in mito_list])/n_mito\n",
    "    nuc_recall = sum([p=='nuclear' for p in nuc_list])/n_nuc\n",
    "    sec_recall = sum([p=='secretory' for p in sec_list])/n_sec\n",
    "    recall_list = [cyt_recall, mito_recall, nuc_recall, sec_recall]\n",
    "    recall_matrix[iteration,] = recall_list\n",
    "\n",
    "    # precision computations\n",
    "    cyt_bool_pred = [p=='cytosolic' for p in classification_list] # a boolean of predicted cytosolic\n",
    "    cyt_bool_true = [(p >= cyt_indices[0]) & (p < cyt_indices[1]) for p in np.arange(0,N_ValSet)] # a boolean of true cytosolic\n",
    "    num_predicted_cyt_are_cyt = sum([a & b for a, b in zip(cyt_bool_pred, cyt_bool_true)]) # their and statement\n",
    "    num_pred_cyt = sum(cyt_bool_pred)\n",
    "    cyt_prec = num_predicted_cyt_are_cyt/num_pred_cyt # cytosolic precision\n",
    "\n",
    "    mito_bool_pred = [p=='mitochondrial' for p in classification_list] # a boolean of predicted cytosolic\n",
    "    mito_bool_true = [(p >= mito_indices[0]) & (p < mito_indices[1]) for p in np.arange(0,N_ValSet)] # a boolean of true cytosolic\n",
    "    num_predicted_mito_are_mito = sum([a & b for a, b in zip(mito_bool_pred, mito_bool_true)]) # their and statement\n",
    "    num_pred_mito = sum(mito_bool_pred)\n",
    "    mito_prec = num_predicted_mito_are_mito/num_pred_mito # cytosolic precision\n",
    "\n",
    "    nuc_bool_pred = [p=='nuclear' for p in classification_list] # a boolean of predicted cytosolic\n",
    "    nuc_bool_true = [(p >= nuc_indices[0]) & (p < nuc_indices[1]) for p in np.arange(0,N_ValSet)] # a boolean of true cytosolic\n",
    "    num_predicted_nuc_are_nuc = sum([a & b for a, b in zip(nuc_bool_pred, nuc_bool_true)]) # their and statement\n",
    "    num_pred_nuc = sum(nuc_bool_pred)\n",
    "    nuc_prec = num_predicted_nuc_are_nuc/num_pred_nuc # cytosolic precision\n",
    "\n",
    "    sec_bool_pred = [p=='secretory' for p in classification_list] # a boolean of predicted cytosolic\n",
    "    sec_bool_true = [(p >= sec_indices[0]) & (p < sec_indices[1]) for p in np.arange(0,N_ValSet)] # a boolean of true cytosolic\n",
    "    num_predicted_sec_are_sec = sum([a & b for a, b in zip(sec_bool_pred, sec_bool_true)]) # their and statement\n",
    "    num_pred_sec = sum(sec_bool_pred)\n",
    "    sec_prec = num_predicted_sec_are_sec/num_pred_sec # cytosolic precision\n",
    "\n",
    "    precision_list = [cyt_prec,mito_prec,nuc_prec,sec_prec]\n",
    "    precision_matrix[iteration,] = precision_list\n",
    "\n",
    "recall_95 = [np.percentile(recall_matrix[:,p],95) for p in [0,1,2,3]]\n",
    "precision_95 = [np.percentile(precision_matrix[:,p],95) for p in [0,1,2,3]]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883b2a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Simulated 95% CI dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f2367",
   "metadata": {},
   "source": [
    "#### Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c4751-fd26-4630-8c7d-06456ac90829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nate's Graph\n",
    "\n",
    "# Define a custom colormap that goes from light red to dark red\n",
    "colors = ['#ffefea','#900000']\n",
    "n_bins = 100  # Number of bins for the colormap\n",
    "cmap_name = 'blue_white_red'\n",
    "ColorMap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "line_color = 'black'\n",
    "\n",
    "# Loop through the precision and recall data for each epoch\n",
    "#   determine when the distance for each precision-recall point is beyond the threshold from a previous point\n",
    "#   if it is, record the distance and say the precision-recall pair should be plotted\n",
    "#   if it is not, maintain the same prior precision-recall point for the class on the next comparison\n",
    "#   the result should be a new column, 'PlotEpoch' stating where the distances from successive plotted points are longer than the stated threshold\n",
    "i = 0\n",
    "d_cut_off = 0.2\n",
    "BestEpochCounter = 0\n",
    "M=[1,1,1,1] # a multiplier vector - one entry per class - that identifies how many points past is the prior point for the distance comparison\n",
    "for row in PrecisionRecallDF.index:\n",
    "    if i < 4:\n",
    "        PrecisionRecallDF.loc[row,'Distance'] = 0\n",
    "        PrecisionRecallDF.loc[row,'PlotEpoch'] = True\n",
    "    else:\n",
    "        j=i%4\n",
    "        CurrentPrecision = PrecisionRecallDF.loc[row,'Precision']\n",
    "        CurrentRecall = PrecisionRecallDF.loc[row,'Recall']\n",
    "        PriorPrecision = PrecisionRecallDF.iloc[i-4*M[j]]['Precision'] # the multiplier is multiplied by 4 because the prior point for the same class is 4 points back\n",
    "        PriorRecall = PrecisionRecallDF.iloc[i-4*M[j]]['Recall']\n",
    "        Distance = ((CurrentPrecision-PriorPrecision)**2+(CurrentRecall-PriorRecall)**2)**0.5\n",
    "        OnBestEpoch = PrecisionRecallDF.loc[i,'Epoch'] == Best_Epoch\n",
    "\n",
    "        if Distance < d_cut_off:\n",
    "            M[j] = M[j]+1\n",
    "            PrecisionRecallDF.loc[row,'Distance'] = 0\n",
    "            PrecisionRecallDF.loc[row,'PlotEpoch'] = False\n",
    "        else:\n",
    "            M[j] = 1\n",
    "            PrecisionRecallDF.loc[row,'Distance'] = Distance\n",
    "            PrecisionRecallDF.loc[row,'PlotEpoch'] = True\n",
    "\n",
    "        # Once your on the best epoch you can stop searching for points to plot\n",
    "        if OnBestEpoch:\n",
    "            BestEpochCounter = BestEpochCounter+1\n",
    "            PrecisionRecallDF.loc[row,'PlotEpoch'] = True\n",
    "        if BestEpochCounter == 4:\n",
    "            break\n",
    "    i = i+1\n",
    "\n",
    "# subset the dataframe by each class\n",
    "PrecRecall_Cyt = PrecisionRecallDF.loc[PrecisionRecallDF.loc[:,'Class']==0,:]\n",
    "PrecRecall_Mito = PrecisionRecallDF.loc[PrecisionRecallDF.loc[:,'Class']==1,:]\n",
    "PrecRecall_Nuc = PrecisionRecallDF.loc[PrecisionRecallDF.loc[:,'Class']==2,:]\n",
    "PrecRecall_Sec = PrecisionRecallDF.loc[PrecisionRecallDF.loc[:,'Class']==3,:]\n",
    "\n",
    "# extract the points to plot\n",
    "PrecRecall_Cyt_Plot = PrecRecall_Cyt.loc[PrecRecall_Cyt.loc[:,'PlotEpoch'],:]\n",
    "PrecRecall_Mito_Plot = PrecRecall_Mito.loc[PrecRecall_Mito.loc[:,'PlotEpoch'],:]\n",
    "PrecRecall_Nuc_Plot = PrecRecall_Nuc.loc[PrecRecall_Nuc.loc[:,'PlotEpoch'],:]\n",
    "PrecRecall_Sec_Plot = PrecRecall_Sec.loc[PrecRecall_Sec.loc[:,'PlotEpoch'],:]\n",
    "\n",
    "# store the data frames of points to plot in a dictionary\n",
    "compartment_class = ['Cytosolic', 'Mitochondria', 'Nuclear', 'Secretory']\n",
    "PrecRecal_Dict = {compartment_class[0]:PrecRecall_Cyt_Plot,compartment_class[1]:PrecRecall_Mito_Plot,compartment_class[2]:PrecRecall_Nuc_Plot,compartment_class[3]:PrecRecall_Sec_Plot}\n",
    "\n",
    "# set up the plot area\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7.75, 8))\n",
    "\n",
    "# make the 4 plots\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    class_metrics = PrecRecal_Dict[compartment_class[i]]\n",
    "\n",
    "    # Normalize epoch for color mapping\n",
    "    norm = plt.Normalize(class_metrics['Epoch'].min(), class_metrics['Epoch'].max())\n",
    "\n",
    "    # Scatter plot with gradient color based on epoch\n",
    "    for idx, row in class_metrics.iterrows():\n",
    "        color = ColorMap(norm(row['Epoch']))\n",
    "        edge_color = 'black'\n",
    "        ax.scatter(row['Recall'], row['Precision'], s=100, color=color, edgecolors=edge_color, linewidths=1)\n",
    "\n",
    "    # Connect the dots with colored lines\n",
    "    ax.plot(class_metrics['Recall'], class_metrics['Precision'], color='black', linewidth=1)\n",
    "\n",
    "    # Plot the 0th and 950th precision and recall as shaded area\n",
    "    #ax.axhline(y=0, color=line_color, linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=precision_95[i], color=line_color, linestyle='--', linewidth=1, alpha=0.5)\n",
    "    #ax.axvline(x=0, color=line_color, linestyle='--', linewidth=1)\n",
    "    ax.axvline(x=recall_95[i], color=line_color, linestyle='--', linewidth=1, alpha=0.5)\n",
    "    #ax.fill_betweenx([0, precision_95[i], 0, recall_95[i]], color=colormaps[i](0.3), alpha=0.3)\n",
    "\n",
    "    # Fill the entire area between hline and vline\n",
    "    ax.fill_betweenx([0, 1], 0, recall_95[i], color='gray', alpha=0.1)\n",
    "    ax.fill_between([0, 1], 0, precision_95[i], color='gray', alpha=0.1)\n",
    "\n",
    "    FontSizeBig=18\n",
    "    FontSizeSmall=14\n",
    "    ax.set_title(compartment_class[i],fontsize=FontSizeBig)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    #ax.set_xlabel('Recall (Sensitivity)',fontsize=FontSizeBig)\n",
    "    #ax.set_ylabel('Precision\\n(Positive Predictive Value)',fontsize=FontSizeBig)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Add in a scatter dot to label the precision and recall of testing set\n",
    "    ax.scatter(J_recall[i], J_precision[i], s=100, color='red', edgecolors='red', linewidths=1, marker='s')\n",
    "    ax.scatter(M_recall[i], M_precision[i], s=100, color='black', edgecolors='black', linewidths=1, marker='s')\n",
    "\n",
    "    # Create custom legend handles\n",
    "    krug_handle = mlines.Line2D([], [], color=colormaps[i](0.5), marker='o', linestyle='None', markersize=10, label='Krug validation set')\n",
    "    johansson_handle = mlines.Line2D([], [], color='red', marker='s', linestyle='None', markersize=10, label='Johansson test set')\n",
    "    mertins_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None', markersize=10, label='Mertins test set')\n",
    "\n",
    "    # Add a legend\n",
    "    #ax.legend(handles=[krug_handle, johansson_handle, mertins_handle], loc='upper right',fontsize=FontSizeSmall)\n",
    "    \n",
    "    # Set tick mark label font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    # Set aspect ratio to be equal (make x and y axes equal length)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # Set tick marks to be every 0.2 units\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(0.2))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure5 = fig.savefig(figure_folder_path + f'figure5_precision_recall_{Run_Name}_{Time_Stamp}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa00903",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c99c1-5762-4620-a2d2-5040b2b7179b",
   "metadata": {},
   "source": [
    "### Summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a5098-9243-4b22-8706-439d3edb1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Image, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the parameters\n",
    "parameters = {\n",
    "    'Run Name': Run_Name,\n",
    "    'Seed': Seed,\n",
    "    'Set': Set,\n",
    "    'Canvas size': Canvas_Size,\n",
    "    'Validation Fraction': Validation_Fraction,\n",
    "    'Standard deviation in add-noise transformation': StandardDeviation,\n",
    "    'Model': Model,\n",
    "    'Weight Decay (L2)': Weight_Decay,\n",
    "    'Momentum': Momentum if optimizer_name == 'SGD_weight' else 'None',\n",
    "    'Loss Function': LossFunc_Name,\n",
    "    'Optimizer': optimizer_name,\n",
    "    'Learning Rate': Learner_rate,\n",
    "    'LR Scheduler?': LrScheduler,\n",
    "    'Label Smoothing': Label_Smoothing,\n",
    "    'Class Weights': LossWeights if ApplyClassWeightToLoss else 'None',\n",
    "    'Batch Size': batch,\n",
    "    'Epoch': epochs,\n",
    "    'Best Epoch': Best_Epoch,\n",
    "    'Transformed?': TransformOrNot,\n",
    "    'Training sample balanced?': ImbalanceSampler,\n",
    "    'Best Validation accuracy': round(val_accuracy, 3),\n",
    "    'Validation macro-avg F1 score': round(val_f1, 3),\n",
    "    'Validation precision': np.round(precision, 3),\n",
    "    'Validation recall': np.round(recall, 3),\n",
    "    'Johansson Test accuracy with best model': round(J_test_accuracy, 3),\n",
    "    'Johansson precision': np.round(J_precision, 3),\n",
    "    'Johansson recall': np.round(J_recall, 3),\n",
    "    'Johansson F1 score': round(J_test_f1, 3),\n",
    "    'Mertins Test accuracy with best model': round(M_test_accuracy, 3),\n",
    "    'Mertins precision': np.round(M_precision, 3),\n",
    "    'Mertins recall': np.round(M_recall, 3),\n",
    "    'Mertins F1 score': round(M_test_f1, 3),\n",
    "}\n",
    "\n",
    "# Define the PDF file\n",
    "summary_file = Path(summary_folder_path+f'parameter_summary_{Run_Name}_{Time_Stamp}.pdf')\n",
    "doc = SimpleDocTemplate(str(summary_file), pagesize=letter)\n",
    "\n",
    "# Define the content\n",
    "content = []\n",
    "\n",
    "# Define a custom style with a specific font size\n",
    "styles = getSampleStyleSheet()\n",
    "custom_style = ParagraphStyle(\n",
    "    'CustomStyle',\n",
    "    parent=styles['Normal'],\n",
    "    fontSize=10,  # Specify the font size\n",
    ")\n",
    "caption_style = ParagraphStyle(\n",
    "    'CaptionStyle',\n",
    "    parent=styles['Normal'],\n",
    "    fontSize=10,  # Specify the font size for captions\n",
    ")\n",
    "\n",
    "# Add the parameter summary to the content\n",
    "content.append(Paragraph('Parameter Summary:', styles['Title']))\n",
    "for key, value in parameters.items():\n",
    "    content.append(Paragraph(f'{key}: {value}', custom_style))\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "# Save the figures and include them in the content\n",
    "figure1.savefig(Path(figure_folder_path+f'figure1_loss_{Run_Name}_{Time_Stamp}.png'), dpi=300, bbox_inches='tight')\n",
    "figure2.savefig(Path(figure_folder_path+f'figure2_accuracy_{Run_Name}_{Time_Stamp}.png'), dpi=300, bbox_inches='tight')\n",
    "content.append(Paragraph('Figures:', styles['Title']))\n",
    "\n",
    "img1 = Image(Path(figure_folder_path+f'figure1_loss_{Run_Name}_{Time_Stamp}.png'))\n",
    "img1.drawHeight = 6*inch*img1.drawHeight / img1.drawWidth\n",
    "img1.drawWidth = 6*inch\n",
    "content.append(img1)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img2 = Image(Path(figure_folder_path+f'figure2_accuracy_{Run_Name}_{Time_Stamp}.png'))\n",
    "img2.drawHeight = 6*inch*img2.drawHeight / img2.drawWidth\n",
    "img2.drawWidth = 6*inch\n",
    "content.append(img2)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img3 = Image(Path(figure_folder_path+f'figure3_val_cm_best_model_{Run_Name}_{Time_Stamp}.png'))\n",
    "img3.drawHeight = 6*inch*img3.drawHeight / img3.drawWidth\n",
    "img3.drawWidth = 6*inch\n",
    "content.append(img3)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img5 = Image(Path(figure_folder_path+f'figure5_precision_recall_{Run_Name}_{Time_Stamp}.png'))\n",
    "img5.drawHeight = 6*inch*img5.drawHeight / img5.drawWidth\n",
    "img5.drawWidth = 6*inch\n",
    "content.append(img5)\n",
    "\n",
    "# Build the PDF\n",
    "doc.build(content)\n",
    "\n",
    "# Print a message indicating that the parameter summary and figures have been saved\n",
    "print('Parameter summary and figures have been saved.')\n",
    "display(HTML('<a href=\"{}\" target=\"_blank\">Click here to download {}</a>'.format(summary_file, summary_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframes\n",
    "PerformanceDF.to_excel(Path(performance_folder_path + f'PerformanceDF_{Time_Stamp}.xlsx'), index=False)\n",
    "PrecisionRecallDF.to_excel(Path(performance_folder_path + f'PrecisionRecallDF_{Time_Stamp}.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
