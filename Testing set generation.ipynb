{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2a24e3",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7c17f9-bca3-44e5-b665-942a8cc73975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "from pdb import set_trace\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfb7cc",
   "metadata": {},
   "source": [
    "### Set seed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688d65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    rs = RandomState(MT19937(SeedSequence(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe99db0",
   "metadata": {},
   "source": [
    "### File path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45501b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "raw_data_path = '/home/ec2-user/MLNotebook/Datasets/Test set raw data/'\n",
    "output_path = '/home/ec2-user/MLNotebook/Datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb4fd6",
   "metadata": {},
   "source": [
    "### Testing set generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72a402",
   "metadata": {},
   "source": [
    "#### Johansson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b0dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johansson data set\n",
    "# Open the proteomics data and only keep genes (rows) that are fully quantified\n",
    "J_PFP = 'jo_protein_log2.csv' # proteomics file path, normalized by pool, log2 transformed.\n",
    "J_PD = pd.read_csv(raw_data_path+J_PFP)\n",
    "\n",
    "# Data set wrangling\n",
    "J_PD.index = J_PD.loc[:,'Unnamed: 0']\n",
    "J_PD = J_PD.loc[:,J_PD.columns!='Unnamed: 0']\n",
    "J_PD.dropna(inplace=True)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(J_PD.values.flatten().tolist())\n",
    "\n",
    "#Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "J_PD = (J_PD - percentile_low) / (percentile_high - percentile_low)\n",
    "\n",
    "# Open the mRNA data and only keep genes (rows) that are fully quantified\n",
    "J_MFP = 'jo_mrna_dropna.csv' # mRNA file path, gene centric median normalized, log2 transformed\n",
    "J_MD = pd.read_csv(raw_data_path + J_MFP)\n",
    "\n",
    "# Data set wrangling\n",
    "J_MD.index = J_MD.loc[:,'Unnamed: 0']\n",
    "J_MD = J_MD.loc[:,J_MD.columns!='Unnamed: 0']\n",
    "J_MD = J_MD.drop_duplicates()\n",
    "J_MD.dropna(inplace=True)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(J_MD.values.flatten().tolist())\n",
    "\n",
    "#Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "J_MD = (J_MD - percentile_low) / (percentile_high - percentile_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303509c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the localization data\n",
    "LFP = 'SubCellBarcode.MCF7.txt'\n",
    "LD = pd.read_csv(filepath_or_buffer=raw_data_path+LFP,sep='\\t')\n",
    "\n",
    "# Data set wrangling\n",
    "LD.index = LD.loc[:,'Protein']\n",
    "LD = LD.loc[:,LD.columns!='Protein']\n",
    "\n",
    "# Remove unclassified class\n",
    "NotUnclassInd = LD.loc[:,'Localization'] != 'Unclassified'\n",
    "LD = LD.loc[NotUnclassInd,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6826cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johansson\n",
      "7989\n",
      "7989\n",
      "7989\n"
     ]
    }
   ],
   "source": [
    "# Keep only genes (rows) are presented in proteome, mRNA and localization data sets\n",
    "IntersectingGenes = [value for value in J_PD.index if ((value in J_MD.index) & (value in LD.index))]\n",
    "J_PD = J_PD.loc[IntersectingGenes,:]\n",
    "J_MD = J_MD.loc[IntersectingGenes,:]\n",
    "LD = LD.loc[IntersectingGenes,:]\n",
    "\n",
    "# Sanity check for the number of genes in each dataframe\n",
    "print('Johansson')\n",
    "print(len(J_PD.index))\n",
    "print(len(J_MD.index))\n",
    "print(len(LD.index))\n",
    "\n",
    "# Encode the localization labels\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label df\n",
    "J_labels = encoder.fit_transform(LD.values.ravel())\n",
    "\n",
    "# Save gene names for later use\n",
    "J_gene_names = J_PD.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e622a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the labels for use\n",
    "LD.to_csv(Path(output_path+'Johansson_Localization.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed5bdf",
   "metadata": {},
   "source": [
    "#### Mertins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba9ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mertins data set\n",
    "# Open the proteomics data and only keep genes (rows) that are fully quantified\n",
    "M_PFP = 'me_protein_dropna.csv' # proteomics file path, normalized by pool, log2 transformed.\n",
    "M_PD = pd.read_csv(raw_data_path+M_PFP)\n",
    "\n",
    "# Data set wrangling\n",
    "M_PD.index = M_PD.loc[:,'Unnamed: 0']\n",
    "M_PD = M_PD.loc[:,M_PD.columns!='Unnamed: 0']\n",
    "M_PD.dropna(inplace=True)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(M_PD.values.flatten().tolist())\n",
    "\n",
    "#Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "M_PD = (M_PD - percentile_low) / (percentile_high - percentile_low)\n",
    "\n",
    "# Open the mRNA data and only keep genes (rows) that are fully quantified\n",
    "M_MFP = 'me_rna_dropna.csv' # mRNA file path, gene centric median normalized, log2 transformed\n",
    "M_MD = pd.read_csv(raw_data_path + M_MFP)\n",
    "\n",
    "# Data set wrangling\n",
    "M_MD.index = M_MD.loc[:,'Unnamed: 0']\n",
    "M_MD = M_MD.loc[:,M_MD.columns!='Unnamed: 0']\n",
    "M_MD = M_MD.drop_duplicates()\n",
    "M_MD.dropna(inplace=True)\n",
    "\n",
    "# Put values of each column in the DataFrame into a list\n",
    "values = np.sort(M_MD.values.flatten().tolist())\n",
    "\n",
    "#Find the 2.5 and 97.5 percentile\n",
    "percentile_high = np.percentile(values, 97.5)\n",
    "percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "# Use the percentile for normalization\n",
    "M_MD = (M_MD - percentile_low) / (percentile_high - percentile_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89aa2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the localization data\n",
    "LFP = 'SubCellBarcode.MCF7.txt'\n",
    "LD = pd.read_csv(filepath_or_buffer=raw_data_path+LFP,sep='\\t')\n",
    "\n",
    "# Data set wrangling\n",
    "LD.index = LD.loc[:,'Protein']\n",
    "LD = LD.loc[:,LD.columns!='Protein']\n",
    "\n",
    "# Remove unclassified class\n",
    "NotUnclassInd = LD.loc[:,'Localization'] != 'Unclassified'\n",
    "LD = LD.loc[NotUnclassInd,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d283e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mertin\n",
      "5076\n",
      "5076\n",
      "5076\n"
     ]
    }
   ],
   "source": [
    "# Keep only genes (rows) are presented in proteome, mRNA and localization data sets\n",
    "IntersectingGenes = [value for value in M_PD.index if ((value in M_MD.index) & (value in LD.index))]\n",
    "M_PD = M_PD.loc[IntersectingGenes,:]\n",
    "M_MD = M_MD.loc[IntersectingGenes,:]\n",
    "LD = LD.loc[IntersectingGenes,:]\n",
    "\n",
    "# Sanity check for the number of genes in each dataframe\n",
    "print('Mertin')\n",
    "print(len(M_PD.index))\n",
    "print(len(M_MD.index))\n",
    "print(len(LD.index))\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label df\n",
    "M_labels = encoder.fit_transform(LD.values.ravel())\n",
    "\n",
    "# Save gene names for later use\n",
    "M_gene_names = M_PD.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce36883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the labels for use\n",
    "LD.to_csv(Path(output_path+'Mertins_Localization.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bac864",
   "metadata": {},
   "source": [
    "### Bayesian inference to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbd95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Set = 'Protein' # Change it to 'Protein' if only generating proteome synthetic data\n",
    "Bayesian = True\n",
    "Canvas_Size = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa6400",
   "metadata": {},
   "source": [
    "#### Johansson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bfbc6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.13 GiB for an array with shape (15, 7989, 7989) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m PD_bgm \u001b[38;5;241m=\u001b[39m BayesianGaussianMixture(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, \n\u001b[1;32m     10\u001b[0m     weight_concentration_prior_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_process\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     11\u001b[0m     weight_concentration_prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \n\u001b[1;32m     12\u001b[0m random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;66;03m# Assuming the maximum number of clusters in dataset is 25\u001b[39;00m\n\u001b[1;32m     13\u001b[0m J_PD_T \u001b[38;5;241m=\u001b[39m J_PD\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 14\u001b[0m \u001b[43mPD_bgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ_PD_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate X new synthetic tumors, result is an array\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Set \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtein\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_base.py:186\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_base.py:241\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_verbose_msg_init_beg(init)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_init:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mif\u001b[39;00m do_init \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower_bound_\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_base.py:145\u001b[0m, in \u001b[0;36mBaseMixture._initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnimplemented initialization method \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_params\n\u001b[1;32m    143\u001b[0m     )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_bayesian_mixture.py:524\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._initialize\u001b[0;34m(self, X, resp)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_weights(nk)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_means(nk, xk)\n\u001b[0;32m--> 524\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_precisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_bayesian_mixture.py:577\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._estimate_precisions\u001b[0;34m(self, nk, xk, sk)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_precisions\u001b[39m(\u001b[38;5;28mself\u001b[39m, nk, xk, sk):\n\u001b[1;32m    562\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the precisions parameters of the precision distribution.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m        'spherical' : (n_components,)\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_wishart_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtied\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_wishart_tied\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_wishart_diag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspherical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_wishart_spherical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[1;32m    586\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.11/site-packages/sklearn/mixture/_bayesian_mixture.py:608\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._estimate_wishart_full\u001b[0;34m(self, nk, xk, sk)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Warning : in some Bishop book, there is a typo on the formula 10.63\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# `degrees_of_freedom_k = degrees_of_freedom_0 + Nk` is\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# the correct formula\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees_of_freedom_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees_of_freedom_prior_ \u001b[38;5;241m+\u001b[39m nk\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components):\n\u001b[1;32m    611\u001b[0m     diff \u001b[38;5;241m=\u001b[39m xk[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_prior_\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 7.13 GiB for an array with shape (15, 7989, 7989) and data type float64"
     ]
    }
   ],
   "source": [
    "# Johansson Bayesian inference\n",
    "# Ensure reproducibility\n",
    "set_seed(43)\n",
    "\n",
    "if Bayesian:\n",
    "\n",
    "    if Set == 'Protein' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        PD_bgm = BayesianGaussianMixture(n_components=15, \n",
    "            weight_concentration_prior_type='dirichlet_process', \n",
    "            weight_concentration_prior=0.5, \n",
    "        random_state=42) # Assuming the maximum number of clusters in dataset is 25\n",
    "        J_PD_T = J_PD.T\n",
    "        PD_bgm.fit(J_PD_T)\n",
    "    \n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'Protein':\n",
    "            J_synthetic_PD, _ = PD_bgm.sample(int(Canvas_Size*Canvas_Size-J_PD.columns.size))\n",
    "        elif Set == 'Protein + mRNA':\n",
    "            J_synthetic_PD, _ = PD_bgm.sample(int((Canvas_Size*Canvas_Size-J_PD.columns.size*2)/2))\n",
    "    \n",
    "        # Transpose back before merging\n",
    "        J_synthetic_PD = J_synthetic_PD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        J_synthetic_PD = pd.DataFrame(J_synthetic_PD.tolist(), index=J_PD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        J_PD = pd.concat([J_PD, J_synthetic_PD], axis=1)\n",
    "\n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        \n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(J_PD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(J_PD.columns) == Canvas_Size*Canvas_Size\n",
    "\n",
    "        print(f'{Set} - Bayesian Johansson testing PD: {len(J_PD.columns)}')\n",
    "      \n",
    "    if Set == 'mRNA' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        MD_bgm = BayesianGaussianMixture(n_components=25, \n",
    "            weight_concentration_prior_type='dirichlet_process', \n",
    "            weight_concentration_prior=0.5, \n",
    "        random_state=43) # Assuming the maximum number of clusters in dataset is 25\n",
    "        J_MD_T = J_MD.T\n",
    "        MD_bgm.fit(J_MD_T)\n",
    "\n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'mRNA':\n",
    "            J_synthetic_MD, _ = MD_bgm.sample(int(Canvas_Size*Canvas_Size-J_MD.columns.size))\n",
    "        if Set == 'Protein + mRNA':\n",
    "            J_synthetic_MD, _ = MD_bgm.sample(int((Canvas_Size*Canvas_Size-J_MD.columns.size*2)/2))\n",
    "\n",
    "        # Transpose back before merging\n",
    "        J_synthetic_MD = J_synthetic_MD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        J_synthetic_MD = pd.DataFrame(J_synthetic_MD.tolist(), index=J_MD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        J_MD = pd.concat([J_MD, J_synthetic_MD], axis=1)\n",
    "         \n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(J_MD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(J_MD.columns) == Canvas_Size*Canvas_Size\n",
    "        print(f'{Set} - Bayesian Johansson testing MD: {len(J_MD.columns)}')\n",
    "\n",
    "else:\n",
    "    print('No synthetic data generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2182046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the synthetic data\n",
    "if Set == 'Protein + mRNA':\n",
    "    J_PD.to_csv(Path(output_path + 'J_prot+mRNA_PD_synthetic.csv'))\n",
    "    J_MD.to_csv(Path(output_path + 'J_prot+mRNA_MD_synthetic.csv'))\n",
    "elif Set == 'Protein':\n",
    "    J_PD.to_csv(Path(output_path + 'J_prot_PD_synthetic.csv'))\n",
    "elif Set == 'mRNA':\n",
    "    J_MD.to_csv(Path(output_path + 'J_prot_MD_synthetic.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeded15",
   "metadata": {},
   "source": [
    "#### Mertins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb994107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mertins Bayesian inference\n",
    "set_seed(Seed)\n",
    "\n",
    "if Bayesian:\n",
    "\n",
    "    if Set == 'Protein' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        PD_bgm = BayesianGaussianMixture(n_components=25, \n",
    "            weight_concentration_prior_type='dirichlet_process', \n",
    "            weight_concentration_prior=0.5, \n",
    "        random_state=42) # Assuming the maximum number of clusters in dataset is 25\n",
    "        M_PD_T = M_PD.T\n",
    "        PD_bgm.fit(M_PD_T)\n",
    "    \n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'Protein':\n",
    "            M_synthetic_PD, _ = PD_bgm.sample(int(Canvas_Size*Canvas_Size-M_PD.columns.size))\n",
    "        elif Set == 'Protein + mRNA':\n",
    "            M_synthetic_PD, _ = PD_bgm.sample(int((Canvas_Size*Canvas_Size-M_PD.columns.size*2)/2))\n",
    "    \n",
    "        # Transpose back before merging\n",
    "        M_synthetic_PD = M_synthetic_PD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        M_synthetic_PD = pd.DataFrame(M_synthetic_PD.tolist(), index=M_PD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        M_PD = pd.concat([M_PD, M_synthetic_PD], axis=1)\n",
    "\n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        \n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(M_PD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(M_PD.columns) == Canvas_Size*Canvas_Size\n",
    "\n",
    "        print(f'{Set} - Bayesian Mertin testing PD: {len(M_PD.columns)}')\n",
    "      \n",
    "    if Set == 'mRNA' or Set == 'Protein + mRNA':\n",
    "        # Fit the dataset to Bayesian Gaussian Mixture Model\n",
    "        MD_bgm = BayesianGaussianMixture(n_components=25, \n",
    "            weight_concentration_prior_type='dirichlet_process', \n",
    "            weight_concentration_prior=0.5, \n",
    "        random_state=43) # Assuming the maximum number of clusters in dataset is 25\n",
    "        M_MD_T = M_MD.T\n",
    "        MD_bgm.fit(M_MD_T)\n",
    "\n",
    "        # Generate X new synthetic tumors, result is an array\n",
    "        if Set == 'mRNA':\n",
    "            M_synthetic_MD, _ = MD_bgm.sample(int(Canvas_Size*Canvas_Size-M_MD.columns.size))\n",
    "        if Set == 'Protein + mRNA':\n",
    "            M_synthetic_MD, _ = MD_bgm.sample(int((Canvas_Size*Canvas_Size-MD.columns.size*2)/2))\n",
    "\n",
    "        # Transpose back before merging\n",
    "        M_synthetic_MD = M_synthetic_MD.T\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        M_synthetic_MD = pd.DataFrame(M_synthetic_MD.tolist(), index=M_MD.index)\n",
    "\n",
    "        # Merge the synthetic data with the original data\n",
    "        M_MD = pd.concat([M_MD, M_synthetic_MD], axis=1)\n",
    "         \n",
    "        # Sanity check for the number of tumors in each dataframe\n",
    "        if Set == 'Protein + mRNA':\n",
    "            assert len(M_MD.columns) == Canvas_Size*Canvas_Size // 2\n",
    "        else:\n",
    "            assert len(M_MD.columns) == Canvas_Size*Canvas_Size\n",
    "        print(f'{Set} - Bayesian Mertin testing MD: {len(M_MD.columns)}')\n",
    "\n",
    "else:\n",
    "    print('No synthetic data generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd2de3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "if Set == 'Protein + mRNA':\n",
    "    M_PD.to_csv(Path(output_path + 'M_prot+mRNA_PD_synthetic.csv'))\n",
    "    M_MD.to_csv(Path(output_path + 'M_prot+mRNA_MD_synthetic.csv'))\n",
    "elif Set == 'Protein':\n",
    "    M_PD.to_csv(Path(output_path + 'M_prot_PD_synthetic.csv'))\n",
    "elif Set == 'mRNA':\n",
    "    M_MD.to_csv(Path(output_path + 'M_prot_MD_synthetic.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
